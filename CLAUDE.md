# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

**AI Coding Agent** - An intelligent CLI coding assistant powered by LangChain and LangGraph, supporting multiple LLM providers (Ollama local, Claude API, Google Gemini, or hybrid smart routing).

**Key Purpose**: Specialized coding assistant for Python, TypeScript, and database technologies (PostgreSQL, MySQL, MongoDB, Snowflake, ClickHouse, Redis, etc.)

**Current Status**: ‚úÖ Production-ready - 285 tests passing, LangGraph agent with Claude Sonnet 4, full tool calling support, RAG system with URL tracking, CrawlAI integration, autonomous documentation crawling, and parallel test infrastructure

## Architecture

### Multi-Provider LLM System

The agent uses LangChain's provider abstraction to support three modes:

1. **Ollama mode** (`LLM_PROVIDER=ollama`): Self-hosted models (local or cloud-deployed)
2. **Claude mode** (`LLM_PROVIDER=claude`): Anthropic API - **Currently Active** ‚úÖ
3. **Gemini mode** (`LLM_PROVIDER=gemini`): Google Gemini API
4. **Hybrid mode** (`LLM_PROVIDER=hybrid`): Smart routing between providers based on query keywords

**Current Model**: `claude-sonnet-4-20250514` (Claude Sonnet 4) - Latest model with **native tool calling support** ‚úÖ. Provides excellent code generation, tool execution, and reasoning capabilities. Alternative models: `claude-3-5-sonnet-20241022`, `gemini-2.5-flash` (free tier limited).

**Smart Routing Logic** (src/agent/agent.py:115-125): In hybrid mode, queries containing keywords like "architecture", "design pattern", "refactor", "optimize", "security", "best practice", "review", "compare" are automatically routed to Claude for better quality. Other queries use Ollama.

### Core Components

- **CodingAgent** (src/agent/agent.py): Main agent class that wraps LangChain's RunnableWithMessageHistory
- **Prompt System** (src/agent/prompts.py): System prompt with database expertise
- **CLI Interface** (src/cli/main.py): Rich terminal UI with markdown rendering
- **Settings** (src/config/settings.py): Pydantic-based configuration with .env support
- **Tools System** (src/agent/tools/): 13 tools ready for agent integration ‚úÖ
  - File operations: ReadFileTool, WriteFileTool, ListDirectoryTool, SearchCodeTool
  - Smart file operations: SmartReadFileTool, UpdateFileSectionTool (for large files)
  - Git tools: GitStatusTool, GitDiffTool, GitCommitTool, GitBranchTool, GitLogTool
  - RAG search: RagSearchTool for semantic codebase search
  - Web crawling: CrawlAndIndexTool for autonomous documentation crawling ‚úÖ NEW
- **RAG System** (src/rag/): Complete FAISS-based semantic search pipeline ‚úÖ
  - Embeddings: sentence-transformers/all-MiniLM-L6-v2 with singleton caching
  - Indexing: File discovery with gitignore, AST-based chunking, FAISS IndexFlatL2
  - Retrieval: Semantic search with similarity threshold filtering and ranking
- **Web Crawler** (src/rag/web_crawler.py): CrawlAI integration for documentation indexing ‚úÖ NEW
  - Async web scraping with Playwright
  - Markdown extraction and cleaning
  - Auto-save to crawled_docs/ directory
  - Ready for RAG indexing integration

### Current Architecture Evolution

**Phase 1 (Complete) - Simple Chain**:
```python
chain = ChatPromptTemplate | LLM
chain_with_history = RunnableWithMessageHistory(chain, get_session_history, ...)
```

**Phase 2 (Complete) - ReAct Agent**:
```python
agent = create_react_agent(llm, tools, prompt)
agent_executor = AgentExecutor(agent, tools, memory=memory)
```

**Phase 3 (Current) - LangGraph Agent**:
```python
from langgraph.prebuilt import create_react_agent
from langgraph.checkpoint.memory import MemorySaver

agent = create_react_agent(llm, tools, checkpointer=MemorySaver())
```

The agent has evolved through three phases:
1. **Simple ConversationChain**: Basic Q&A with memory
2. **ReAct Agent**: Added tool use with reasoning
3. **LangGraph Agent**: Advanced workflow orchestration with state management

**Current Capabilities**:
- Reading files in current directory
- Writing/modifying files with backups and diff preview
- Listing directory contents
- Searching code with regex
- Semantic codebase search using RAG
- **Autonomous documentation crawling and indexing** ‚úÖ NEW - Agent can crawl web docs and add to RAG index
- Multi-step reasoning with tool composition
- State persistence across sessions (via LangGraph checkpointing)

### LangChain Integration

Conversation history is stored in memory using `InMemoryChatMessageHistory` keyed by session ID.

### Provider Switching

When using hybrid mode, the agent rebuilds the chain with the selected LLM on each request (src/agent/agent.py:165-172). This allows dynamic provider selection while maintaining conversation history.

## Development Commands

### Quick Start

```bash
# Setup
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt requirements-dev.txt
cp .env.example .env

# Run agent
python main.py

# Run tests
pytest

# Format & lint
black src/ && ruff check src/
```

### Testing

```bash
# All tests (16 test files, 195+ tests passing)
pytest

# Quick run (specific tests)
pytest tests/test_agent.py -v                    # Agent basics
pytest tests/test_langgraph_agent.py -v         # LangGraph integration
pytest tests/test_tools/ -v                      # All tool tests (147 tests)
pytest tests/test_rag/ -v                        # All RAG tests (48 tests)

# Individual tool tests
pytest tests/test_tools/test_read_file.py        # 40+ tests
pytest tests/test_tools/test_write_file.py       # 30+ tests
pytest tests/test_tools/test_list_directory.py   # 50+ tests
pytest tests/test_tools/test_search_code.py      # 40+ tests
pytest tests/test_tools/test_file_ops_security.py # 30+ security tests

# RAG tests
pytest tests/test_rag/test_embeddings.py          # 31 tests
pytest tests/test_rag/test_indexer_integration.py # 9 tests
pytest tests/test_rag/test_retriever.py           # 8 tests

# Coverage
pytest --cov=src --cov-report=html
```

### Code Quality

```bash
# Formatting
black src/

# Linting
ruff check src/

# Type checking
mypy src/

# All three at once
black src/ && ruff check src/ && mypy src/
```

### Local Development (Ollama)

```bash
# Install Ollama: https://ollama.ai

# Start Ollama service
ollama serve

# Pull model
ollama pull qwen2.5-coder:1.5b

# List models
ollama list

# Test model
ollama run qwen2.5-coder:1.5b "Write a Python function to calculate fibonacci"
```

### Debugging

```bash
# Check Ollama service
curl http://localhost:11434/api/tags

# Verify agent config
python -c "from src.config.settings import settings; print(f'Provider: {settings.LLM_PROVIDER}, Model: {settings.MODEL_NAME}, Tools: {settings.ENABLE_TOOLS}')"

# Check dependencies
pip list | grep -E "langchain|faiss|sentence-transformers|torch"

# Check FAISS index
ls -lh ~/.ai-agent/faiss_index/ 2>/dev/null || echo "No index yet"

# Test agent directly
python -c "from src.agent.agent import CodingAgent; agent = CodingAgent(); print(agent.ask('Hello'))"
```

## Configuration

All configuration is managed through `.env` file (see `.env.example` for template).

**Critical Settings**:
- `LLM_PROVIDER`: Controls which provider(s) to use ("ollama", "claude", "hybrid")
- `MODEL_NAME`: Default is `qwen2.5-coder:1.5b`
- `OLLAMA_BASE_URL`: Change to `https://your-server.com` for cloud deployment
- `ANTHROPIC_API_KEY`: Required for Claude or hybrid mode
- `TEMPERATURE`: Set to 0.1 for focused code generation (default: 0.1)

**New Tool & RAG Settings** (src/config/settings.py:47-51):
- `ENABLE_TOOLS`: Feature flag for tool-based agent (default: True, **requires compatible model**)
- `ENABLE_FILE_OPS`: Enable file operation tools (default: True)
- `ENABLE_RAG`: Enable RAG semantic search (default: True)
- `VECTOR_DB`: "faiss" (using FAISS instead of ChromaDB for Python 3.13 compatibility)
- `FAISS_INDEX_PATH`: Path to FAISS index storage (~/.ai-agent/faiss_index)
- `EMBEDDING_MODEL`: "sentence-transformers/all-MiniLM-L6-v2" (local embeddings)
- `MAX_FILE_SIZE_MB`: File size limit for operations (default: 10MB)
- `FILE_BACKUP_ENABLED`: Automatic backups before writes (default: True)

**Cloud Migration**: To migrate from local to cloud Ollama, only `OLLAMA_BASE_URL` needs to be changed. All code remains identical.

### Model Requirements for Tool Use

**IMPORTANT**: The LangGraph tool-based agent requires models with **native function/tool calling support**. Not all models support this feature!

**‚úÖ Compatible Models (Tool Mode Works)**:
- **Claude**: claude-3-haiku-20240307, claude-3-5-sonnet (recommended for quality)
- **Gemini**: gemini-1.5-flash, gemini-1.5-pro
- **GPT**: gpt-4, gpt-4-turbo, gpt-3.5-turbo
- **Ollama**: Check model card - must explicitly support function calling

**‚ùå Incompatible Models (Tool Mode Fails)**:
- **qwen2.5-coder:1.5b** (current default) - No tool calling support
- **llama3.2:1b** - Too small, no tool calling
- **codellama:7b** - No native tool calling support
- Most Ollama models < 13B parameters

**Workarounds**:
1. **Use Claude or Gemini for tool mode**: Set `LLM_PROVIDER=claude` and add `ANTHROPIC_API_KEY` to .env
2. **Use hybrid mode**: Set `LLM_PROVIDER=hybrid` to route complex queries to Claude
3. **Disable tools**: Set `ENABLE_TOOLS=False` to use simple conversational mode
4. **Future**: Wait for prompt-based ReAct implementation (doesn't require native tool calling)

**Testing Tool Support**:
```bash
# Test if your model supports tools
python -c "
from src.agent.agent import CodingAgent
agent = CodingAgent(provider='your-provider', temperature=0.1)
response = agent.ask('What is 2+2?')
print('SUCCESS' if '4' in response.content else 'FAILED - check model compatibility')
"
```

## Code Patterns

### Adding New CLI Commands

CLI commands are handled in the main loop (src/cli/main.py:95-137). To add a command:

1. Add command check in the main loop
2. Implement command logic
3. Update help text in `print_welcome()` and `print_help()`

### Extending System Prompts

Database-specific knowledge is in `src/agent/prompts.py`. The `DATABASE_TIPS` dictionary contains specialized knowledge that could be used for RAG in the future.

### Error Handling Pattern

The agent's `ask()` method (src/agent/agent.py:127-187) catches exceptions and provides context-specific error messages:
- Ollama errors suggest checking if `ollama serve` is running
- API errors suggest checking `ANTHROPIC_API_KEY`

### Provider Selection Logic

Force a specific provider for a query:
```python
response = agent.ask("your query", force_provider="claude")
```

This overrides hybrid mode routing.

### Tool Implementation Pattern (In Development)

Tools use Pydantic models for type-safe input validation:
```python
from pydantic import BaseModel, Field
from langchain.tools import BaseTool

class ReadFileInput(BaseModel):
    path: str = Field(description="Path to file to read")

class ReadFileTool(BaseTool):
    name = "read_file"
    description = "Read contents of a file"
    args_schema = ReadFileInput

    def _run(self, path: str) -> str:
        # Implementation with path validation
        ...
```

## Important Implementation Details

### Memory Management

- Conversation history is stored per-session in `self.store` dictionary (src/agent/agent.py:91)
- Default session ID is "default" (src/agent/agent.py:92)
- Clear history with `agent.clear_history()` which empties the session's InMemoryChatMessageHistory
- History is NOT persisted; it's lost when the application exits

### Chain Rebuilding in Hybrid Mode

In hybrid mode, the chain is rebuilt on each request to swap the LLM (src/agent/agent.py:166-172). This is necessary because LangChain chains are immutable. The session history store is shared across chain instances, maintaining conversation context.

### Model Info Display

The CLI's `info` command calls `agent.get_model_info()` which returns different fields based on the provider mode. Note: The CLI implementation (src/cli/main.py:36-49) expects specific keys like "model", "base_url", "deployment" which don't match the hybrid mode output structure - this is a bug that should be fixed.

### RAG System Architecture (In Development)

**Vector Database**: FAISS (Facebook AI Similarity Search)
- **Why FAISS over ChromaDB**: Python 3.13 compatibility issues with ChromaDB's chroma-hnswlib dependency
- **Trade-offs**: Manual save/load (1-2 lines of code) vs ChromaDB's automatic persistence
- **Benefits**: Pre-built wheels for Python 3.13, faster search, production-proven, lower memory footprint

**Embedding Model**: sentence-transformers/all-MiniLM-L6-v2
- Local execution (no API costs)
- Small (80MB) and fast
- Good semantic similarity for code
- Aligns with "local-first" philosophy

**Chunking Strategy**:
- Function-level chunks for Python/JS/TS using AST parsing
- Include context (imports, class definitions, decorators, docstrings)
- Fallback: Sliding window (500 chars, 50 overlap) for non-parseable files
- Metadata: file path, language, function/class name, line numbers, timestamps, file hash

**Indexing**:
- Storage: `~/.ai-agent/faiss_index/`
- What: All `.py`, `.js`, `.ts`, `.md`, `.json`, `.yaml`, `.yml`, `.txt` files
- Exclude: `.venv/`, `node_modules/`, `__pycache__/`, `.git/`
- Incremental updates using file hashes
- Index versioning for schema changes
- Batch processing (32 files at a time)
- Respect .gitignore patterns

### ReAct Pattern Validation

**Critical Pre-Implementation Test** (scripts/test_react_format.py):
- Validated that qwen2.5-coder:1.5b can follow ReAct format
- Result: PASSED ‚úÖ (3/3 format compliance, 2/3 tool identification)
- This confirms the model can use the Thought/Action/Action Input/Observation pattern

## Testing Strategy

Tests are in the `tests/` directory with comprehensive coverage:

### Unit Tests (tests/test_agent.py)
- **Pre-flight checks**: Validates Ollama service is running and model is available
- **Agent initialization**: Tests agent creation with different providers
- **Configuration validation**: Tests settings and environment variables
- **Basic functionality**: Tests simple query responses
- **Conversation memory**: Tests history retention across queries
- **Error handling**: Tests Ollama connection failures, API key issues
- **Integration tests**: Tests with real Ollama instance

### Tool Tests (tests/test_tools/) - ‚úÖ COMPLETE
- **test_read_file.py**: 40+ tests for read functionality, encoding, error handling
- **test_list_directory.py**: 50+ tests for directory listing, formatting, permissions
- **test_write_file.py**: 30+ tests for write functionality, backups, diffs, confirmations
- **test_search_code.py**: 40+ tests for regex search, file filtering, exclusions
- **test_file_ops_security.py**: 30+ security tests (path traversal, symlinks, size limits)
- **test_crawl_and_index.py**: 12+ tests for autonomous web crawling and indexing ‚úÖ NEW
- **Total**: 159 tests, all passing

### RAG Tests (tests/test_rag/) - ‚úÖ COMPLETE
- **test_embeddings.py**: 31 tests for model loading, caching, embedding generation ‚úÖ COMPLETE
- **test_indexer_integration.py**: 9 integration tests for file discovery, chunking, index building ‚úÖ COMPLETE
- **test_retriever.py**: 8 tests for semantic search, ranking, tool integration ‚úÖ COMPLETE
  - Retriever initialization and index loading
  - IndexNotFoundError handling
  - Semantic search with relevant/irrelevant queries
  - top_k parameter behavior and threshold filtering
  - RagSearchTool formatted output and error handling
- **Total**: 48 tests, all passing

### Future Tests (Planned - Agent Integration Phase)
- Agent tool execution with ReAct pattern
- Tool selection and routing logic
- End-to-end agent workflows with all 5 tools
- Performance benchmarks for RAG search at scale
- Multi-turn conversations with tool use

### Testing with LangChain

Use `FakeListLLM` for unit tests (avoids real LLM calls):
```python
from langchain_community.llms.fake import FakeListLLM

def test_agent():
    fake_llm = FakeListLLM(responses=["Test response"])
    agent = CodingAgent(llm=fake_llm)
    assert agent.ask("test") == "Test response"
```

### Test Count Verification

The project currently has:
- **Tool Tests**: 6 test files in `tests/test_tools/` (159+ assertions including crawl_and_index)
- **RAG Tests**: 5 test files in `tests/test_rag/` (87 assertions: 48 core + 39 crawler)
  - Core RAG: embeddings (31), indexer (9), retriever (8)
  - Crawler: tracker unit tests (31), integration tests (8)
- **Agent Tests**: Multiple files in `tests/` for agent, CLI, providers (39+ assertions)
- **Total**: 285 tests across 19 test files with comprehensive coverage
- **Parallel Execution**: 3-4x faster with pytest-xdist on multi-core systems

Note: Some tests require Ollama to be running (`ollama serve`) and the model to be available (`ollama pull qwen2.5-coder:1.5b`).

## Known Issues & Gotchas

1. **CLI info command bug**: The `print_model_info()` function expects keys that don't exist in hybrid/claude mode. Needs refactoring to handle all provider modes.

2. **Temperature default mismatch**: `.env.example` shows TEMPERATURE=0.1, but settings.py defaults to 1.0. This is confusing. (Note: Now standardized to 0.1)

3. **Hybrid mode prints to stdout**: Provider selection messages use `print()` instead of the Rich console (src/agent/agent.py:160, 162), which breaks the UI styling.

4. **Python 3.13 ChromaDB Incompatibility**: ChromaDB's chroma-hnswlib dependency fails to build on Python 3.13 due to compilation issues. **Solution**: Using FAISS instead, which has pre-built wheels and better Python 3.13 support.

5. **Large Dependencies**: PyTorch and sentence-transformers add ~3GB of dependencies. This is expected for local embeddings and is a trade-off for avoiding API costs.

6. **RAG Retriever Similarity Score** (src/rag/retriever.py:113): The similarity calculation `1.0 / (1.0 + dist)` works but could be refined. For normalized vectors with IndexFlatL2, the formula `1 - (dist¬≤ / 2)` would be more mathematically accurate for cosine similarity. Current implementation is functional but produces slightly different score ranges.

7. **RAG Retriever Caching**: Unlike embeddings.py which uses `@lru_cache`, the retriever creates new instances on each call. This means repeated searches reload the index from disk. Consider adding singleton pattern for production use.

8. **LangGraph Tool Calling Limitations** (**CRITICAL**): The LangGraph agent implementation using `create_react_agent` requires models with **native tool/function calling support**. The current default model `qwen2.5-coder:1.5b` does NOT support this and will fail to properly execute tools.
   - **Working Models**: Claude (claude-3-haiku+), GPT-4, Gemini, larger Ollama models with tool support
   - **Not Working**: qwen2.5-coder:1.5b, llama3.2:1b, most small Ollama models
   - **Workaround**: Set `ENABLE_TOOLS=False` in .env to use simple conversational mode
   - **Future Fix**: Implement prompt-based ReAct agent for small models (doesn't require native tool calling)

## Current Implementation Status

### ‚úÖ Completed (Step 0-2)

1. **ReAct Pattern Validation** (scripts/test_react_format.py)
   - Validated qwen2.5-coder:1.5b can follow Thought/Action/Observation format
   - Result: 3/3 format compliance, 2/3 tool identification
   - Decision: Proceed with ReAct agent implementation

2. **Foundation Setup**
   - Created directory structure:
     - `src/agent/tools/` - Tool implementations
     - `src/rag/` - RAG system components
     - `tests/test_tools/` - Tool tests
     - `tests/test_rag/` - RAG tests
   - Created `src/utils/logging.py` with structured logging for file ops and RAG
   - Updated `src/config/settings.py` with comprehensive RAG and file operations configuration

3. **Dependencies Installed**
   - `faiss-cpu==1.9.0.post1` (vector database)
   - `sentence-transformers==2.3.1` (embeddings)
   - `torch>=2.0.0` (required by sentence-transformers)
   - `pathspec==0.12.1` (gitignore patterns)
   - `tqdm==4.66.1` (progress indicators)

4. **Configuration Updates**
   - Model changed from `codellama:7b` to `qwen2.5-coder:1.5b` throughout codebase
   - Added tool configuration (ENABLE_TOOLS, ENABLE_FILE_OPS, ENABLE_RAG)
   - Added RAG configuration (FAISS paths, embedding model, chunking params)
   - Added file operations configuration (size limits, allowed extensions, backups)
   - Added indexing configuration (batch size, exclude patterns, gitignore support)
   - Added logging configuration (file operations logging, log levels)

5. **Testing Infrastructure**
   - Created comprehensive `tests/test_agent.py` with Ollama pre-flight checks
   - All tests passing with qwen2.5-coder:1.5b model

6. **Step 2: File Operation Tools** (src/agent/tools/file_ops.py) ‚úÖ COMPLETE
   - ‚úÖ `ReadFileTool` - Read file with path validation, size limits, encoding fallback
   - ‚úÖ `WriteFileTool` - Write with backup & diff preview, confirmation flag
   - ‚úÖ `ListDirectoryTool` - List files and folders with metadata
   - ‚úÖ `SearchCodeTool` - Regex code search with file filtering and exclude patterns
   - ‚úÖ Security: Path validation (prevent traversal), size limits, backups, comprehensive logging
   - ‚úÖ Tests: Created comprehensive test suites:
     - `tests/test_tools/test_read_file.py` - 40+ tests for read functionality
     - `tests/test_tools/test_list_directory.py` - 50+ tests for directory listing
     - `tests/test_tools/test_write_file.py` - 30+ tests for write functionality
     - `tests/test_tools/test_search_code.py` - 40+ tests for code search
     - `tests/test_tools/test_file_ops_security.py` - 30+ security tests
   - ‚úÖ Factory functions: `get_read_file_tool()`, `get_list_directory_tool()`, `get_write_file_tool()`, `get_search_code_tool()`

7. **Step 3: RAG System - Embeddings** (src/rag/embeddings.py) ‚úÖ COMPLETE
   - ‚úÖ Singleton model loading using `@functools.lru_cache` decorator
   - ‚úÖ `get_embeddings()` - Convert text chunks to 384-dim vectors with batching support
   - ‚úÖ `get_model()` - Access singleton SentenceTransformer instance
   - ‚úÖ Error handling: `ModelLoadError`, `EmbeddingGenerationError` with detailed messages
   - ‚úÖ Features:
     - Automatic model caching (loaded once, reused)
     - Batch processing with configurable batch size
     - Normalization control (required for FAISS cosine similarity)
     - Progress bar support for large batches
     - Unicode and code snippet support
     - Returns JSON-serializable Python lists
   - ‚úÖ Tests: `tests/test_rag/test_embeddings.py` - 31 tests covering:
     - Model loading and singleton pattern
     - Embedding generation and consistency
     - Input validation and error handling
     - Batch processing (1 to 1000 texts)
     - Integration and typical workflows
   - ‚úÖ All 31 tests passing

8. **Step 3: RAG System - Indexing** (src/rag/) ‚úÖ COMPLETE
   - ‚úÖ **Part A: Setup and Filtering** (`indexer_utils.py`) - COMPLETE
     - `FileInfo` dataclass with path, size, mtime, content hash
     - `load_gitignore_patterns()` - Load .gitignore + hardcoded exclusions (.venv, node_modules, etc.)
     - `file_discovery_generator()` - Iterator with security checks, gitignore filtering, extension filtering
     - Leverages existing `validate_path()` for security
   - ‚úÖ **Part B: Chunking** (`chunker.py`) - COMPLETE
     - `CodeChunker` - AST-based chunking for Python using tree-sitter
       - Extracts functions and classes with context (imports, class signatures)
       - Handles decorators, docstrings, method nesting
       - Graceful fallback if tree-sitter unavailable
     - `TextChunker` - Sliding window fallback for non-code files
       - Configurable chunk size (500 chars) and overlap (50 chars)
       - Line-accurate tracking for metadata
       - Minimum chunk size to avoid tiny chunks
     - `chunk_file()` - Orchestration function that selects appropriate chunker
       - Uses CodeChunker for .py files (with fallback)
       - Uses TextChunker for .md, .txt, and other files
       - Singleton pattern for chunker instances (cached)
     - `CodeChunk` dataclass with content, file_path, start_line, end_line, chunk_type, language, metadata
   - ‚úÖ **Part C: Indexing Orchestration** (`indexer.py`) - COMPLETE
     - `build_index()` - Main indexing pipeline with 5 steps:
       1. File discovery using `file_discovery_generator()`
       2. Chunking all files with per-file error handling
       3. Batch embedding generation with progress bars
       4. FAISS index creation (IndexFlatL2 for cosine similarity)
       5. Index persistence (index.faiss + chunks_metadata.json + index_info.json)
     - `save_index()` - Persist FAISS index and metadata
     - `load_index()` - Load FAISS index and metadata from disk
     - `index_exists()` - Check if index files exist
     - Error handling: `IndexBuildError`, `IndexSaveError`, `IndexError`
     - Features:
       - Progress bars with tqdm integration
       - Per-file error handling (don't crash on one bad file)
       - Batch processing (configurable batch size, default 32)
       - Comprehensive logging with statistics
   - ‚úÖ Tests: `tests/test_rag/test_indexer_integration.py` - 9 integration tests covering:
     - File discovery with gitignore patterns
     - Python code chunking (with tree-sitter fallback)
     - Text file chunking (markdown)
     - Index building and saving
     - Index loading from disk
     - Index existence checking
     - End-to-end pipeline with test codebase
   - ‚úÖ All 9 integration tests passing

### ‚úÖ Completed (Step 3: RAG System - Retrieval)

9. **Step 3: RAG System - Retrieval** ‚úÖ COMPLETE
   - ‚úÖ **Retriever** (`retriever.py`) - COMPLETE
     - `SemanticRetriever` class - Load FAISS index and perform semantic searches
     - `search()` method - Convert query to embedding, search index, return ranked results
     - Similarity threshold filtering (default 0.5) to ensure relevance
     - Error handling: `IndexNotFoundError` with helpful messages
     - Logging: Debug and info level for search operations
     - Factory function: `get_retriever()` for dependency injection
     - **Note**: Similarity score calculation uses `1.0 / (1.0 + dist)` formula
   - ‚úÖ **RAG Search Tool** (`rag_search.py`) - COMPLETE
     - `RagSearchTool` - LangChain BaseTool wrapper for retriever
     - Pydantic input validation with `RagSearchInput` schema
     - Formatted output for LLM consumption (file path, line numbers, score, content)
     - Error handling: IndexNotFoundError, generic exceptions
     - Async support (delegates to sync)
     - Factory function: `get_rag_search_tool()` for dependency injection
     - **Note**: Hardcoded top_k=5, could be made configurable
   - ‚úÖ Tests: `tests/test_rag/test_retriever.py` - 8 tests covering:
     - Retriever initialization and index loading
     - IndexNotFoundError when index missing
     - Semantic search with relevant/irrelevant queries
     - No results for irrelevant query (threshold filtering)
     - top_k parameter behavior
     - RagSearchTool formatted output
     - RagSearchTool error handling
   - ‚úÖ All 8 tests passing
   - **Technical Review**: Grade B+ - Functional and well-tested, minor improvements recommended:
     - Consider caching retriever instances to avoid repeated disk I/O
     - Similarity score calculation could be refined for normalized vectors
     - top_k could be made configurable via tool input

### ‚úÖ Phase 2: Tools & RAG System Complete

**Summary**: All file operation tools and RAG components are implemented, tested, and ready for agent integration.
- **Total Tests**: 285 tests passing (159 tool tests + 87 RAG + 39 agent/integration)
- **Test Coverage**: Unit tests + integration tests + security tests + parallel execution
- **Components**: 13 tools (6 file ops + 1 RAG + 1 crawl + 5 git tools) + embeddings + indexing + retrieval + URL tracking
- **Status**: ‚úÖ Fully integrated with Claude Sonnet 4

---

### ‚ö†Ô∏è Completed with Limitations: Agent Integration

10. **Step 4: Agent Refactoring** ‚úÖ COMPLETE
    - ‚úÖ Upgraded from ConversationChain to LangGraph ReAct agent using `create_react_agent`
    - ‚úÖ Registered all 13 tools (6 file ops + 1 RAG + 1 crawl + 5 git tools)
    - ‚úÖ Implemented MemorySaver for state persistence via checkpointing
    - ‚úÖ Cleaned up unused imports and fixed missing methods
    - ‚úÖ Fixed dependency conflicts (langgraph 0.2.76 + langchain-core 0.3.63)
    - ‚úÖ Migrated to Claude Sonnet 4 for native tool calling support
    - ‚úÖ Added tests: `test_agent_react.py` and `test_langgraph_agent.py`

11. **Step 5: CLI Integration** (PARTIAL - IN PROGRESS)
    - ‚è≥ Add `index` command for codebase indexing (TODO)
    - ‚è≥ Add `info` subcommand to show index statistics (TODO)
    - ‚è≥ Add tool execution indicators (show which tool is running) (TODO)
    - ‚è≥ Update help messages with new commands (TODO)
    - ‚è≥ Add progress indicators for indexing (TODO)

12. **Step 6: Testing & Validation** ‚úÖ COMPLETE
    - ‚úÖ Unit tests for file operations (COMPLETE - 6 test files including crawl_and_index)
    - ‚úÖ Unit tests for embeddings (COMPLETE)
    - ‚úÖ Integration tests for indexer (COMPLETE)
    - ‚úÖ Unit tests for retriever (COMPLETE)
    - ‚úÖ Crawler tests: URL tracking (31 tests) + integration (8 tests)
    - ‚úÖ Security tests for path traversal, symlinks (COMPLETE)
    - ‚úÖ LangGraph agent tests (COMPLETE - test_langgraph_agent.py)
    - ‚úÖ End-to-end workflow tests with Claude Sonnet 4 (COMPLETE)
    - ‚úÖ Parallel test infrastructure with pytest-xdist
    - ‚úÖ All 285 tests passing

---

### ‚úÖ Phase 3: Production Fixes & CrawlAI Integration (December 2024)

13. **Critical Bug Fixes** ‚úÖ COMPLETE (Dec 8, 2024)
    - **Problem**: Gemini Flash 2.5 broke the codebase with undefined types and API changes
    - ‚úÖ Fixed undefined `Step` type error (removed unused planning methods)
    - ‚úÖ Fixed missing `ENABLE_GIT_TOOLS` setting
    - ‚úÖ Fixed RAG indexer signature (reverted to tuple return)
    - ‚úÖ Fixed Pydantic v2 configuration syntax (SettingsConfigDict)
    - ‚úÖ Updated test assertions for Claude's intelligent responses
    - ‚úÖ Identified and documented shell env var override issue
    - **Result**: All tests passing, production-ready ‚úÖ

14. **Model Migration to Claude Sonnet 4** ‚úÖ COMPLETE (Dec 8, 2024)
    - ‚úÖ Migrated from `qwen2.5-coder:1.5b` to `claude-sonnet-4-20250514`
    - ‚úÖ **Native tool calling now works!** - Verified with live tests
    - ‚úÖ Updated all test fixtures to use configured provider from .env
    - ‚úÖ Fixed test assertions to handle Claude's summarization behavior
    - ‚úÖ Fixed search tests to differentiate actual errors from code containing "Error"
    - **Benefits**:
      - Tool execution actually works (no more JSON output instead of tool calls)
      - Intelligent, context-aware responses
      - Better code generation quality
    - **Note**: Requires valid ANTHROPIC_API_KEY and available credits

15. **CrawlAI Web Documentation Integration** ‚úÖ COMPLETE (Dec 8, 2024)
    - ‚úÖ Installed CrawlAI 0.7.7 with Playwright
    - ‚úÖ Created `src/rag/web_crawler.py` module (178 lines)
    - ‚úÖ Implemented `WebDocumentationCrawler` class
      - Async web scraping with Playwright browser automation
      - Markdown extraction and cleaning
      - Auto-save to `~/.ai-agent/crawled_docs/` directory
      - Error handling and logging
    - ‚úÖ Added crawler configuration to settings:
      - `ENABLE_WEB_CRAWLING`, `CRAWLER_HEADLESS`, `CRAWLER_VERBOSE`
      - `CRAWLED_DOCS_PATH`, `CRAWLER_USER_AGENT`
    - ‚úÖ Tested successfully: Crawled Python argparse docs (128KB markdown in 1.26s)
    - ‚úÖ Added `crawl4ai>=0.7.7` to requirements.txt

16. **CrawlAndIndexTool - Autonomous Documentation Crawling** ‚úÖ COMPLETE (Dec 8, 2024)
    - ‚úÖ Created `src/agent/tools/crawl_and_index.py` module (250 lines)
    - ‚úÖ Implemented `CrawlAndIndexTool` - LangChain BaseTool for agent
      - Crawls web documentation URLs on demand
      - Chunks markdown content using existing chunker
      - Adds chunks to existing FAISS index (incremental updates)
      - Returns detailed summary of crawling and indexing operation
    - ‚úÖ Features:
      - Async crawling with proper error handling
      - Automatic integration with existing RAG index
      - Creates new index if none exists
      - Saves markdown files to crawled_docs directory
      - Progress logging and detailed status reporting
    - ‚úÖ Registered with agent (13th tool)
    - ‚úÖ Added tool summary formatting for agent responses
    - ‚úÖ Created comprehensive test suite: `tests/test_tools/test_crawl_and_index.py`
      - 12 tests covering initialization, crawling, chunking, indexing, error handling
      - Mock-based unit tests (no network required)
      - Integration test available (skipped by default)
    - ‚úÖ Updated regression tests for 13-tool count
    - ‚úÖ Initial test suite: 12 tests passing
    - **Agent Capability**: Can now autonomously crawl and index documentation when asked!
    - **Example**: "Crawl and index the FastAPI dependency injection docs"
    - **Future Enhancements**:
      - Add CLI `/crawl <url>` command for manual control ‚úÖ COMPLETED (see section 17)
      - Add batch documentation crawling script
      - Add sitemap support for crawling entire doc sites

17. **URL Tracking & Deduplication System** ‚úÖ COMPLETE (Dec 8, 2024)
    - ‚úÖ Created `src/rag/crawl_tracker.py` module (250 lines)
    - ‚úÖ Implemented `CrawlTracker` class for persistent URL tracking
      - JSON-based storage at `~/.ai-agent/crawled_docs/crawled_urls.json`
      - MD5 content hashing for change detection
      - Tracks URL, date, content hash, chunk count, file path, title
      - Deduplication: Skips re-crawling unchanged content
      - Statistics: Total URLs, chunks, content size, average chunks per URL
    - ‚úÖ Created `CrawlRecord` dataclass for metadata storage
    - ‚úÖ Integrated tracking with `CrawlAndIndexTool`
      - Automatic tracking of all crawled URLs
      - Smart detection of content changes (re-index only if changed)
      - Clear user feedback for duplicates vs updates
    - ‚úÖ Factory function: `get_crawl_tracker()` for global singleton
    - ‚úÖ **Benefits**: Saves time and API costs by avoiding duplicate crawls
    - ‚úÖ Comprehensive test suite: `tests/test_rag/test_crawl_tracker.py`
      - 31 unit tests covering all functionality
      - Tests for init, hashing, detection, record management, stats, persistence
      - Parallel-safe using `tmp_path` fixtures

18. **CLI Commands for Manual Crawling** ‚úÖ COMPLETE (Dec 8, 2024)
    - ‚úÖ Added `crawl <url>` command to `src/cli/main.py`
      - Directly crawls and indexes a URL without agent interaction
      - Bypasses LangGraph recursion limits
      - Color-coded output (green=success, blue=duplicate, red=error)
      - Shows detailed statistics (chunks indexed, title)
    - ‚úÖ Added `crawled` command to show crawl history
      - Rich table display with URL, title, chunks, date
      - Sorted by crawl date (most recent first)
      - Shows total chunks indexed across all URLs
      - Empty state message if nothing crawled yet
    - ‚úÖ Updated help messages and welcome screen
    - ‚úÖ **User Experience**: Fast, direct control without triggering agent loops

19. **Parallel Test Infrastructure** ‚úÖ COMPLETE (Dec 8, 2024)
    - ‚úÖ Added `pytest-xdist==3.5.0` to requirements-dev.txt
      - Enables parallel test execution across CPU cores
      - 3-4x faster on multi-core systems
    - ‚úÖ Created `pytest.ini` configuration
      - Test markers: unit, integration, slow, parallel, serial
      - Asyncio mode: auto
      - Strict markers and clean output
    - ‚úÖ Created `run_tests.sh` executable script (100+ lines)
      - 10+ test modes: all, parallel, crawler, unit, integration, coverage, etc.
      - Color-coded output and progress indicators
      - Auto-detects CPU count for optimal parallelization
    - ‚úÖ Created `TESTING.md` comprehensive guide (250+ lines)
      - Quick start, parallel execution guidelines
      - Coverage reports, debugging techniques
      - Best practices for parallel-safe tests
      - CI/CD integration examples
    - ‚úÖ Crawler integration tests: `tests/test_rag/test_crawl_integration.py`
      - 8 tests for complete crawl ‚Üí track ‚Üí index workflow
      - Tests for deduplication, change detection, statistics
      - Parameterized multi-URL tests for parallel execution
    - ‚úÖ **Test Results**: 39 crawler tests (31 unit + 8 integration) all passing in <2 seconds
    - ‚úÖ **Total Test Count**: 285 tests (246 existing + 39 crawler)

20. **Circular Import Fix - Path Validation Refactoring** ‚úÖ COMPLETE (Dec 8, 2024)
    - **Problem**: Circular dependency between RAG and agent modules
      - `src/rag/indexer_utils.py` imported from `src/agent/tools/file_ops.py`
      - `src/agent/agent.py` imported from `src/agent/tools/crawl_and_index.py`
      - `crawl_and_index.py` imported from `src/rag/indexer.py` (circular!)
    - ‚úÖ Created `src/utils/path_validation.py` shared module
      - Moved `validate_path()`, `check_file_size()`, `CWD` constant
      - Moved exceptions: `FileOperationError`, `PathValidationError`, `FileSizeError`
    - ‚úÖ Updated `src/agent/tools/file_ops.py` to import from shared module
    - ‚úÖ Updated `src/rag/indexer_utils.py` to import from shared module
    - ‚úÖ Removed unused imports from `src/rag/web_crawler.py`
    - ‚úÖ **Result**: Clean dependency tree, no circular imports, all tests passing

21. **Index Management System** ‚úÖ COMPLETE (Dec 8, 2024)
    - ‚úÖ Created `src/rag/index_manager.py` module (340 lines)
    - ‚úÖ Implemented 4 main functions for index operations:
      - `get_index_info()` - Comprehensive index statistics
        - Returns: exists, total_chunks, total_files, crawled_urls, index_size_mb, last_updated
        - Breakdown by source (local vs crawled) and language
        - Embedding model and version tracking
      - `rebuild_index()` - Rebuild entire FAISS index from scratch
        - Force reindex with progress indicators
        - Returns statistics: success, chunks_indexed, files_processed, errors, duration
      - `clean_index()` - Remove orphaned chunks (deleted files)
        - Checks file existence, rebuilds index with valid chunks only
        - Returns: success, chunks_removed, chunks_remaining, files_removed
      - `search_index()` - Direct index search with configurable parameters
        - Configurable threshold, top_k, source filtering (local/crawled)
        - Useful for testing and debugging search relevance
    - ‚úÖ CLI integration in `src/cli/main.py`:
      - `index` or `index --info` - Show index statistics (3 rich tables)
      - `index --rebuild` - Rebuild index with confirmation prompt
      - `index --clean` - Clean orphaned chunks
      - `index --search "query" [--threshold 0.3] [--top-k 10] [--source local/crawled]`
    - ‚úÖ Rich console output with tables and colored panels
    - ‚úÖ Custom exception: `IndexNotFoundError` with helpful messages
    - ‚úÖ Comprehensive test suite: `tests/test_rag/test_index_manager.py`
      - 10 tests covering all functions and error cases
      - Mock-based unit tests for clean isolation
      - Tests for info retrieval, rebuild, cleaning, search, error handling
    - ‚úÖ **Bug Fixes During Testing**:
      - Fixed `build_index()` parameter mismatch (root_dir ‚Üí cwd, added force_reindex)
      - Fixed `load_index()` unpacking (returns 2 values, not 3)
      - Fixed `SemanticRetriever()` initialization (requires settings parameter)
      - Fixed search parameter name (threshold ‚Üí similarity_threshold)
      - Added `--threshold` CLI parameter for adjustable search strictness
    - ‚úÖ **Benefits**: Complete control over RAG index lifecycle, debugging tools, maintenance operations

22. **Batch Documentation Crawling** ‚úÖ COMPLETE (Dec 8, 2024)
    - ‚úÖ Created `src/rag/batch_crawler.py` module (340 lines)
    - ‚úÖ Implemented `BatchCrawler` class for efficient multi-URL crawling:
      - `crawl_batch()` - Parallel crawl with concurrency limits
        - asyncio semaphore for max concurrent operations (default: 5)
        - Rate limiting with configurable delay (default: 1 second)
        - Duplicate detection using CrawlTracker
        - Progress tracking with tqdm
        - Returns `BatchCrawlResult` with detailed statistics
      - `crawl_from_file()` - Read URLs from text file
        - One URL per line, `#` for comments
        - Automatic blank line skipping
      - `crawl_from_sitemap()` - Fetch and parse sitemap.xml
        - Async HTTP fetch with aiohttp
        - XML parsing with ElementTree
        - Namespace handling for standard sitemaps
        - URL filtering support (e.g., only /api/ URLs)
      - `_parse_sitemap()` - XML parsing with namespace support
        - Handles both namespaced and non-namespaced sitemaps
        - Optional URL filtering by substring
    - ‚úÖ `BatchCrawlResult` dataclass with comprehensive stats:
      - total_urls, successful, failed, skipped
      - duration_seconds, urls_crawled, urls_failed, urls_skipped
    - ‚úÖ Convenience functions for simple usage:
      - `batch_crawl_urls()` - Direct batch crawling
      - `batch_crawl_from_file()` - File-based crawling
      - `batch_crawl_from_sitemap()` - Sitemap-based crawling
    - ‚úÖ CLI integration in `src/cli/main.py`:
      - `crawl --batch <file> [--parallel N]` - Batch crawl from file
      - `crawl --sitemap <url> [--filter pattern] [--parallel N]` - Crawl from sitemap
      - Rich table output with colored results
      - Error details displayed in separate table
    - ‚úÖ Updated help messages with new batch crawling commands
    - ‚úÖ Comprehensive test suite: `tests/test_rag/test_batch_crawler.py`
      - 18 tests covering all functionality
      - Tests for initialization, batch crawling, failures, duplicates
      - File-based crawling tests with temp files
      - Sitemap tests with mock HTTP responses
      - XML parsing tests (with/without namespace, filtering, errors)
      - Async context manager mocking for aiohttp
      - Convenience function tests
    - ‚úÖ **Code Quality Improvements**:
      - Removed unused `WebDocumentationCrawler` initialization
      - Simplified cleanup() method (no-op, uses tools directly)
      - Fixed async mock setup for proper `async with` support
    - ‚úÖ **Benefits**:
      - Efficiently crawl entire documentation sites
      - Sitemap support for auto-discovery
      - Rate limiting to respect server limits
      - Parallel execution for speed
      - Skip duplicates to save time/costs
    - ‚úÖ **Total Test Count**: 303 tests (285 existing + 18 batch crawler)

23. **Documentation Crawl Profiles** ‚úÖ COMPLETE (Dec 8, 2024)
    - ‚úÖ Created `src/rag/crawl_profiles.py` module (300 lines)
    - ‚úÖ Implemented `CrawlProfile` dataclass for profile configuration:
      - name, description, urls, sitemap_url, url_filter, max_concurrent
      - Validation: Must have either URLs or sitemap
      - Two profile types: URL list-based or sitemap-based
    - ‚úÖ Implemented `ProfileManager` class for profile management:
      - `get_profile()` - Get profile by name
      - `list_profiles()` - List all profiles (sorted by name)
      - `add_profile()` - Add or update a profile
      - `remove_profile()` - Remove a profile
      - `get_stats()` - Profile statistics (total, sitemap vs URL list)
      - JSON persistence at `~/.ai-agent/crawled_docs/crawl_profiles.json`
    - ‚úÖ **10 Default Profiles** for popular frameworks:
      - **fastapi** - FastAPI docs (sitemap + /tutorial/ filter)
      - **django** - Django core docs (9 key URLs)
      - **react** - React docs (sitemap + /learn/ filter)
      - **vue** - Vue.js docs (8 key URLs)
      - **typescript** - TypeScript handbook (sitemap + /docs/handbook/ filter)
      - **python-stdlib** - Python standard library (sitemap + /library/ filter)
      - **flask** - Flask docs (5 key URLs)
      - **express** - Express.js docs (6 key URLs)
      - **postgresql** - PostgreSQL SQL commands (sitemap + /sql- filter)
      - **langchain** - LangChain docs (sitemap + /docs/ filter)
    - ‚úÖ CLI integration in `src/cli/main.py`:
      - `crawl --profile <name> [--parallel N]` - Crawl using a profile
      - `profiles` - List all available profiles (rich table)
      - `profiles --info <name>` - Show detailed profile information (panel)
      - Profile crawling uses existing batch crawler infrastructure
      - Supports both sitemap and URL list profiles
    - ‚úÖ Rich console output:
      - Profile list table with name, description, type, URL count
      - Profile info panel with detailed configuration
      - Crawl results table with statistics
    - ‚úÖ Global singleton: `get_profile_manager()` for easy access
    - ‚úÖ Comprehensive test suite: `tests/test_rag/test_crawl_profiles.py`
      - 25 tests covering all functionality
      - Tests for CrawlProfile dataclass (4 tests)
      - Tests for ProfileManager (15 tests)
      - Tests for singleton pattern (2 tests)
      - Tests for specific default profiles (4 tests)
      - Mock-based unit tests for clean isolation
    - ‚úÖ **Benefits**:
      - One-command crawling of entire documentation sites
      - Pre-configured for popular frameworks (no URL hunting)
      - Customizable - users can add their own profiles
      - Saves time and ensures complete coverage
      - Profile configurations persist across sessions
    - ‚úÖ **Total Test Count**: 328 tests (303 existing + 25 profiles)

24. **PostgreSQL + pgvector Dual-Mode Storage** ‚ö†Ô∏è IN PROGRESS (Dec 8, 2024)
    - **Goal**: Migrate RAG system from FAISS-only to dual-mode (FAISS + PostgreSQL)
    - **Architecture**: Strategy pattern with abstract `StorageBackend` interface
    - **Priority**: Batch crawling performance with transactional safety
    - **Backward Compatibility**: FAISS remains default (no breaking changes)

    **‚úÖ Phase 1: Foundation (COMPLETE)**
    - ‚úÖ Created `src/rag/storage_backend.py` (260 lines)
      - Abstract `StorageBackend` interface with 6 core methods
      - `SearchResult` dataclass for unified results
      - Custom exceptions: `StorageBackendError`, `IndexNotFoundError`, `IndexBuildError`, `SearchError`
      - Methods: `index_exists()`, `build_index()`, `add_chunks()`, `search()`, `get_stats()`, `clear()`
    - ‚úÖ Created `src/rag/backend_factory.py` (180 lines)
      - `get_storage_backend()` - Singleton factory with lazy loading
      - Selects FAISS or PostgreSQL based on `ENABLE_POSTGRES_STORAGE` flag
      - Helper functions: `reset_backend()`, `get_backend_type()`, `is_backend_initialized()`
    - ‚úÖ Created `scripts/init_postgres_schema.sql` (370 lines)
      - Tables: `code_chunks`, `crawled_urls`, `index_metadata`, `crawl_sessions`
      - pgvector column: `embedding vector(384)` for sentence-transformers/all-MiniLM-L6-v2
      - HNSW indexes for fast vector similarity search (m=16, ef_construction=64)
      - Helper functions: `update_index_stats()`, `search_similar_chunks()`, `truncate_all_tables()`
      - Views: `v_index_statistics`, `v_crawl_statistics`
    - ‚úÖ Updated `src/config/settings.py` with PostgreSQL configuration:
      - `ENABLE_POSTGRES_STORAGE: bool = False` (defaults to FAISS)
      - `DATABASE_URL`, `DB_POOL_SIZE`, `DB_CONNECTION_TIMEOUT`
      - `PGVECTOR_INDEX_TYPE`, `PGVECTOR_HNSW_M`, `PGVECTOR_HNSW_EF_CONSTRUCTION`
      - `BATCH_CRAWL_MAX_CONCURRENT`, `BATCH_CRAWL_SKIP_DUPLICATES`
    - ‚úÖ Updated `requirements.txt`: Added `psycopg2-binary==2.9.9`

    **‚úÖ Phase 3: PostgreSQL Backend (COMPLETE)**
    - ‚úÖ Created `src/rag/postgres_backend.py` (540 lines)
      - `PostgresBackend` class implementing `StorageBackend` interface
      - Connection pooling with `ThreadedConnectionPool` (configurable pool size)
      - `build_index()` - Bulk insert with `execute_batch` (100 chunks/batch)
      - `add_chunks()` - Incremental indexing (INSERT without TRUNCATE)
      - `search()` - pgvector similarity using `<->` operator for L2 distance
      - `get_stats()` - Comprehensive statistics (chunks, files, sizes in MB)
      - `clear()` - TRUNCATE all tables
      - `delete_by_file()` - Delete chunks from specific file
      - Transaction safety: ACID guarantees, automatic rollback on errors
      - Resource management: Context managers, connection cleanup

    **‚è∏Ô∏è Phase 2: FAISS Adapter (PENDING - Waiting for Profiling Work)**
    - ‚è∏Ô∏è Create `src/rag/faiss_backend.py` - Wrap existing FAISS logic
    - ‚è∏Ô∏è Refactor `src/rag/indexer.py` to use `get_storage_backend()`
    - ‚è∏Ô∏è Refactor `src/rag/retriever.py` to use `get_storage_backend()`
    - ‚è∏Ô∏è Run all 328 tests to verify backward compatibility
    - **Why Waiting**: Avoids conflicts with concurrent profiling work on indexer.py/retriever.py

    **üîú Remaining Phases (After Phase 2)**
    - Phase 4: Batch crawler for PostgreSQL (concurrent crawling with transactions)
    - Phase 5: Migration scripts (FAISS ‚Üî PostgreSQL conversion)
    - Phase 6: Integration testing (50+ new tests for PostgreSQL mode)
    - Phase 7: Documentation (setup guide, migration guide, performance comparison)

    **Current Status**:
    - ‚úÖ PostgreSQL backend fully implemented and ready
    - ‚úÖ Dual-mode architecture designed and partially implemented
    - ‚è∏Ô∏è Integration pending (Phase 2) - waiting for profiling work to complete
    - üîß Configuration: `ENABLE_POSTGRES_STORAGE=False` (FAISS mode, default)
    - **Timeline**: ~2 weeks remaining (Phase 2-7) after profiling completes

25. **Multi-Profile Batch Crawling & Stacks** ‚úÖ COMPLETE (Dec 8, 2024)
    - ‚úÖ Enhanced `src/rag/batch_crawler.py` with multi-profile support (~200 lines added)
    - ‚úÖ Implemented **Multi-Profile Crawling**:
      - `crawl_multiple_profiles()` - Crawl multiple profiles sequentially
        - Sequential crawling (one profile at a time for simplicity)
        - Aggregates statistics across all profiles
        - Per-profile error handling (continues on failure)
        - Returns `MultiProfileCrawlResult` with detailed breakdown
      - `ProfileCrawlResult` dataclass - Individual profile results
      - `MultiProfileCrawlResult` dataclass - Aggregated results
        - total_profiles, successful_profiles, failed_profiles
        - total_urls, successful_urls, failed_urls, skipped_urls
        - duration_seconds, profile_results list
    - ‚úÖ Implemented **Predefined Stacks** for common tech stacks:
      - `PROFILE_STACKS` dictionary with 7 stacks:
        - **backend**: fastapi, django, flask, postgresql
        - **frontend**: react, vue, typescript
        - **fullstack**: fastapi, django, react, vue, typescript, postgresql
        - **python**: python-stdlib, fastapi, django, flask
        - **javascript**: react, vue, express, typescript
        - **database**: postgresql
        - **ai**: langchain
      - `crawl_stack()` - Crawl all profiles in a predefined stack
      - `get_stack_profiles()` - Get profile names for a stack
      - `list_stacks()` - List all available stacks
    - ‚úÖ Enhanced **CLI Integration** in `src/cli/main.py`:
      - `crawl --profiles name1,name2,... [--parallel N]` - Multi-profile crawling
      - `crawl --stack <name> [--parallel N]` - Stack crawling
      - `stacks` - List all available stacks
      - `run_multi_profile_crawl()` - Rich console output with tables
      - `run_stack_crawl()` - Stack-specific output
      - `show_stacks()` - Display all stacks in formatted table
    - ‚úÖ Added **Progress Tracking**:
      - tqdm progress bar for profiles (e.g., "Crawling profiles: 2/5")
      - Dynamic description showing current profile being crawled
      - Fallback when tqdm not available
    - ‚úÖ Comprehensive test suite: `tests/test_rag/test_batch_crawler.py`
      - Added `TestMultiProfileCrawling` class with 6 tests
      - Tests for successful multi-profile crawling
      - Tests for handling profile failures gracefully
      - Tests for stack crawling (valid and invalid stacks)
      - Tests for stack utility functions
      - All tests passing
    - ‚úÖ **Benefits**:
      - **Time Saving**: Crawl entire tech stacks in one command
      - **Convenience**: No need to run multiple crawl commands
      - **Standardization**: Predefined stacks ensure consistent documentation coverage
      - **Flexibility**: Can combine profiles or use pre-made stacks
      - **Error Resilience**: Failed profiles don't stop entire batch
      - **Visibility**: Detailed statistics for each profile and overall
    - ‚úÖ **Usage Examples**:
      ```bash
      # Crawl multiple individual profiles
      crawl --profiles fastapi,django,react --parallel 10

      # Crawl a predefined stack
      crawl --stack backend --parallel 5
      crawl --stack fullstack --parallel 10

      # List available stacks
      stacks
      ```
    - ‚úÖ **Total Test Count**: 334 tests (328 existing + 6 multi-profile)

26. **Local PostgreSQL Development Setup** ‚úÖ COMPLETE (Dec 9, 2024)
    - ‚úÖ Created comprehensive Docker-based PostgreSQL setup for team collaboration
    - ‚úÖ **Docker Compose Configuration** (`docker-compose.yml`):
      - PostgreSQL 16 with pgvector extension (`pgvector/pgvector:pg16` image)
      - Automatic schema initialization on first startup
      - Health checks and restart policies
      - Optional pgAdmin 4 service (with `--profile with-pgadmin`)
      - Persistent volumes for data and pgAdmin settings
      - Pre-configured server connections for pgAdmin
      - Isolated network for services
    - ‚úÖ **Database Initialization** (`scripts/`):
      - `init_postgres_schema.sql` - Auto-mounted, runs on first startup (existing, 370 lines)
      - `seed_data.sql` - Optional sample data and startup messages (NEW, 30 lines)
      - `pgadmin_servers.json` - Pre-configured pgAdmin server connection (NEW)
    - ‚úÖ **Utility Scripts** (`scripts/`):
      - `setup_postgres.sh` - One-command setup script (checks Docker, starts services, tests connection)
      - `test_postgres_connection.py` - Comprehensive connection and schema verification
        - Tests: connection, pgvector extension, tables, views, vector operations
        - Detailed output with troubleshooting suggestions
      - `show_postgres_stats.py` - Database statistics dashboard
        - Index stats, crawl stats, table sizes, recent activity, connection info
        - Formatted tables with Rich console output
      - `clean_postgres_db.py` - Interactive database cleaning utility
        - Options: clean all, clean chunks only, vacuum, show stats
        - Safety confirmations for destructive operations
      - All scripts are executable (`chmod +x`)
    - ‚úÖ **Documentation**:
      - `POSTGRES_QUICKSTART.md` - 5-minute quick start for team members (NEW, concise)
      - `docs/POSTGRES_SETUP.md` - Comprehensive setup and troubleshooting guide (NEW, 400 lines)
        - Quick start (3 steps)
        - Connection details and database schema
        - pgAdmin access instructions
        - Common operations (backup, restore, reset, view logs)
        - Troubleshooting section
        - Development workflow guide
        - Security notes (dev vs production)
      - Updated `.env.example` - Added PostgreSQL configuration section (NEW)
        - `ENABLE_POSTGRES_STORAGE`, `DATABASE_URL`, connection pool settings
        - pgvector index configuration (HNSW parameters)
        - Batch crawling settings
        - Quick start instructions in comments
    - ‚úÖ **Connection Details** (for local development):
      - Host: localhost, Port: 5432
      - Database: ai_agent, User: ai_agent_user
      - Password: dev_password_change_in_production (‚ö†Ô∏è dev only!)
      - Connection string: `postgresql://ai_agent_user:dev_password_change_in_production@localhost:5432/ai_agent`
    - ‚úÖ **pgAdmin Access** (optional):
      - URL: http://localhost:5050
      - Login: admin@aiagent.local / admin
      - Pre-configured server: "AI Agent Local"
    - ‚úÖ **Features**:
      - **Zero-Config Setup**: `./scripts/setup_postgres.sh` handles everything
      - **Team Consistency**: Docker ensures identical environment across team members
      - **Isolated Environment**: No conflicts with system PostgreSQL
      - **Persistent Data**: Database survives container restarts
      - **Easy Reset**: `docker-compose down -v` for clean slate
      - **Monitoring Tools**: Stats dashboard and pgAdmin UI
      - **Safety First**: Confirmations for destructive operations
    - ‚úÖ **Benefits for Team**:
      - **Fast Onboarding**: New team members setup in 5 minutes
      - **No Installation Required**: Just Docker (no PostgreSQL install needed)
      - **Consistent Environment**: Same version, extensions, schema for everyone
      - **Easy Debugging**: Logs, stats, and pgAdmin for troubleshooting
      - **Safe Testing**: Local isolated database, can reset anytime
      - **Production Parity**: Same PostgreSQL 16 + pgvector as production
    - ‚úÖ **Usage**:
      ```bash
      # One-line setup
      ./scripts/setup_postgres.sh

      # Or manual
      docker-compose up -d postgres
      python scripts/test_postgres_connection.py
      python scripts/show_postgres_stats.py

      # With pgAdmin
      docker-compose --profile with-pgadmin up -d
      # Access: http://localhost:5050
      ```
    - ‚úÖ **Supports Other Team Member's Work**:
      - Ready for PostgreSQL backend integration (Phase 2)
      - Compatible with existing postgres_backend.py implementation
      - Schema matches init_postgres_schema.sql (370 lines, 4 tables, views, functions)
      - Enables concurrent development and testing
      - No conflicts with FAISS mode (dual-mode architecture)
    - ‚úÖ **Installation & Deployment** (Dec 9, 2024):
      - **Docker Installation**: Completed on Fedora 42
        - Installed Docker 29.0.4 and Docker Compose 2.40.3
        - Configured user group permissions
        - Started and enabled Docker service
      - **PostgreSQL Container**: Successfully deployed and running
        - Container: `ai-agent-postgres` (healthy status)
        - Image: `pgvector/pgvector:pg16` (PostgreSQL 16.11 with pgvector)
        - Port: 5432 (exposed on localhost)
        - Network: `ai-agent-network` (isolated)
        - Volume: `ai-agent-postgres-data` (persistent storage)
      - **Database Schema**: Fully initialized
        - Tables: `code_chunks`, `crawled_urls`, `index_metadata`, `crawl_sessions` ‚úÖ
        - Views: `v_index_statistics`, `v_crawl_statistics` ‚úÖ
        - pgvector extension: Enabled ‚úÖ
        - Helper functions: All created ‚úÖ
        - Vector dimension: 384 (sentence-transformers/all-MiniLM-L6-v2)
      - **Connection Tested**: All verifications passed ‚úÖ
        - Connection successful to localhost:5432
        - Authentication working (ai_agent_user)
        - All tables accessible
        - Vector operations functional
        - Test script: `scripts/test_postgres_connection.py` passed
      - **Dependencies**: Installed
        - `psycopg2-binary==2.9.11` (Python PostgreSQL adapter)
      - **Configuration**: Updated
        - `.env` file updated with `DATABASE_URL`
        - `ENABLE_POSTGRES_STORAGE=false` (default to FAISS, can switch to PostgreSQL)
      - **Issue Resolved**: File permissions
        - Initial auto-init failed due to permission denied on mounted SQL file
        - Resolved by manually running: `docker-compose exec -T postgres psql -U ai_agent_user -d ai_agent < scripts/init_postgres_schema.sql`
        - Schema fully initialized and operational
      - **Status Documents**: Created for team reference
        - `POSTGRES_READY.md` - Deployment summary and quick commands
        - `SETUP_STATUS.md` - Detailed setup checklist
    - ‚úÖ **Current Status** (Dec 9, 2024):
      - üü¢ **OPERATIONAL**: PostgreSQL database is running and ready for use
      - üü¢ **TESTED**: Connection and schema verified
      - üü¢ **DOCUMENTED**: Complete setup and usage guides
      - üü¢ **TEAM-READY**: Other team members can connect immediately
      - ‚è≠Ô∏è **Next Step**: Enable in application by setting `ENABLE_POSTGRES_STORAGE=true` in `.env`
    - ‚úÖ **Access Commands**:
      ```bash
      # Check status
      sudo docker-compose ps postgres

      # Test connection
      python3 scripts/test_postgres_connection.py

      # View statistics
      python3 scripts/show_postgres_stats.py

      # Connect with psql
      sudo docker-compose exec postgres psql -U ai_agent_user -d ai_agent

      # View logs
      sudo docker-compose logs -f postgres
      ```

## Future Enhancements (as documented)

From ARCHITECTURE.md roadmap:

### Phase 2 (Current Development)
- üöß **File Operations**: Read, write, list, search files with safety features
- üöß **RAG System**: FAISS-based semantic codebase search with function-level chunking
- üöß **Tool-Based Agent**: ReAct pattern for autonomous tool use

### Phase 3 (Post-Tool Implementation)
- [ ] **Conversation Persistence**: Redis or Postgres-backed message history
- [ ] **Streaming**: LangChain streaming callbacks for real-time output
- [ ] **Web UI**: FastAPI + LangServe for API deployment
- [ ] **Multi-Agent**: LangGraph for complex workflows
- [ ] **SQL Agent**: LangChain SQL agent for direct database queries

### Phase 4 (Advanced)
- [ ] **Code Execution**: LangChain code execution tool
- [ ] **Git Integration**: Custom LangChain tool for version control
- [ ] **Analytics**: LangSmith for tracing and monitoring

## Project Structure

```
ollama-agentic-project/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ agent/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ agent.py              # Main agent with LangGraph integration
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ prompts.py            # Database knowledge & tool prompts
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ tools/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ __init__.py       # Tool registry
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ file_ops.py       # File operation tools (‚úÖ COMPLETE)
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ rag_search.py     # RAG search tool (‚úÖ COMPLETE)
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ crawl_and_index.py # Web crawling tool (‚úÖ COMPLETE)
‚îÇ   ‚îú‚îÄ‚îÄ cli/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ main.py               # Terminal interface
‚îÇ   ‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ settings.py           # Configuration (updated with RAG & tools)
‚îÇ   ‚îú‚îÄ‚îÄ rag/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py           # RAG module exports (‚úÖ COMPLETE)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ embeddings.py         # Embedding model setup (‚úÖ COMPLETE)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ indexer_utils.py      # File discovery & gitignore (‚úÖ COMPLETE)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ chunker.py            # Code & text chunking (‚úÖ COMPLETE)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ indexer.py            # FAISS index building (‚úÖ COMPLETE)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ retriever.py          # Semantic search (‚úÖ COMPLETE)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ storage_backend.py    # Abstract storage interface (‚úÖ COMPLETE - NEW)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ backend_factory.py    # Backend factory (FAISS/PostgreSQL) (‚úÖ COMPLETE - NEW)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ postgres_backend.py   # PostgreSQL + pgvector backend (‚úÖ COMPLETE - NEW)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ faiss_backend.py      # FAISS backend adapter (‚è∏Ô∏è PENDING - Phase 2)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ web_crawler.py        # CrawlAI web documentation (‚úÖ COMPLETE)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ crawl_tracker.py      # URL tracking & deduplication (‚úÖ COMPLETE)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ index_manager.py      # Index management utilities (‚úÖ COMPLETE)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ batch_crawler.py      # Batch crawling & sitemap support (‚úÖ COMPLETE)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ crawl_profiles.py     # Pre-configured crawl profiles (‚úÖ COMPLETE)
‚îÇ   ‚îî‚îÄ‚îÄ utils/
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îú‚îÄ‚îÄ logging.py            # Structured logging (‚úÖ COMPLETE)
‚îÇ       ‚îî‚îÄ‚îÄ path_validation.py    # Shared path validation (‚úÖ COMPLETE)
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ test_agent.py             # Basic agent tests
‚îÇ   ‚îú‚îÄ‚îÄ test_agent_react.py       # ReAct agent tests
‚îÇ   ‚îú‚îÄ‚îÄ test_langgraph_agent.py   # LangGraph agent tests ‚úÖ NEW
‚îÇ   ‚îú‚îÄ‚îÄ test_cli_info.py          # CLI info command tests
‚îÇ   ‚îú‚îÄ‚îÄ test_gemini_provider.py   # Gemini provider tests
‚îÇ   ‚îú‚îÄ‚îÄ test_tools/               # Tool tests (6 test files)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_read_file.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_list_directory.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_write_file.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_search_code.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_file_ops_security.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ test_crawl_and_index.py
‚îÇ   ‚îî‚îÄ‚îÄ test_rag/                 # RAG tests (8 test files)
‚îÇ       ‚îú‚îÄ‚îÄ test_embeddings.py
‚îÇ       ‚îú‚îÄ‚îÄ test_indexer_integration.py
‚îÇ       ‚îú‚îÄ‚îÄ test_retriever.py
‚îÇ       ‚îú‚îÄ‚îÄ test_crawl_tracker.py       # ‚úÖ (31 tests)
‚îÇ       ‚îú‚îÄ‚îÄ test_crawl_integration.py   # ‚úÖ (8 tests)
‚îÇ       ‚îú‚îÄ‚îÄ test_index_manager.py       # ‚úÖ (10 tests)
‚îÇ       ‚îú‚îÄ‚îÄ test_batch_crawler.py       # ‚úÖ (18 tests)
‚îÇ       ‚îî‚îÄ‚îÄ test_crawl_profiles.py      # ‚úÖ NEW (25 tests)
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ benchmark_models.py       # Model comparison
‚îÇ   ‚îú‚îÄ‚îÄ test_react_format.py      # ReAct validation (‚úÖ PASSED)
‚îÇ   ‚îú‚îÄ‚îÄ init_postgres_schema.sql  # PostgreSQL + pgvector schema (‚úÖ COMPLETE - NEW)
‚îÇ   ‚îú‚îÄ‚îÄ migrate_faiss_to_postgres.py  # Migration tool (üîú Phase 5)
‚îÇ   ‚îî‚îÄ‚îÄ export_postgres_to_faiss.py   # Export tool (üîú Phase 5)
‚îú‚îÄ‚îÄ docs/
‚îÇ   ‚îú‚îÄ‚îÄ ARCHITECTURE.md
‚îÇ   ‚îú‚îÄ‚îÄ DEPLOYMENT_ANALYSIS.md
‚îÇ   ‚îî‚îÄ‚îÄ [other documentation]
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îî‚îÄ‚îÄ ...                       # Test files (see above)
‚îú‚îÄ‚îÄ main.py                       # Entry point
‚îú‚îÄ‚îÄ requirements.txt              # Includes LangGraph, FAISS, embeddings, CrawlAI
‚îú‚îÄ‚îÄ requirements-dev.txt          # Testing and development tools (pytest-xdist)
‚îú‚îÄ‚îÄ pytest.ini                    # Pytest configuration for parallel tests ‚úÖ NEW
‚îú‚îÄ‚îÄ run_tests.sh                  # Test runner script with 10+ modes ‚úÖ NEW
‚îú‚îÄ‚îÄ TESTING.md                    # Comprehensive testing guide ‚úÖ NEW
‚îú‚îÄ‚îÄ .env.example                  # Configuration template
‚îî‚îÄ‚îÄ CLAUDE.md                     # This file
```

## Documentation Files

- **README.md**: User-facing documentation, quick start, deployment guide
- **ARCHITECTURE.md**: Detailed technical architecture, LangChain integration patterns
- **DEPLOYMENT_ANALYSIS.md**: Cloud deployment options (CPU & GPU instances)
- **GPU_DEPLOYMENT_GUIDE.md**: GPU deployment strategy & ROI analysis
- **MULTI_PROVIDER_GUIDE.md**: Guide for multi-provider setup
- **DESIGN_SUMMARY.md**: Quick reference guide
- **EXECUTIVE_DECISION_BRIEF.md**: Management-focused ROI analysis
- **MODEL_COMPARISON_REPORT.md**: Model selection rationale
- **CLAUDE.md**: This file - guidance for Claude Code when working with the codebase

## Key Decisions Made

### Model Selection
- **Decision**: Use qwen2.5-coder:1.5b
- **Rationale**: Local machine limitations, smaller model size, validated ReAct compatibility
- **Impact**: Updated throughout codebase (.env.example, settings.py, tests, scripts)

### Vector Database
- **Decision**: Use FAISS instead of ChromaDB
- **Rationale**: Python 3.13 compatibility (ChromaDB's chroma-hnswlib fails to build)
- **Trade-offs**: Manual save/load (1-2 lines) vs automatic persistence
- **Benefits**: Pre-built wheels, faster, lower memory, production-proven

### Agent Pattern
- **Decision**: LangGraph-based ReAct Agent (upgraded from basic ReAct)
- **Rationale**: LangGraph provides better state management, checkpointing, and workflow control
- **Trade-offs**: Slightly more complex than basic ReAct, but offers production-ready features
- **Future**: Can extend to multi-agent workflows (researcher + coder + reviewer)

### Implementation Approach
- **Decision**: Gradual rollout with feature flags (ENABLE_TOOLS now defaults to True)
- **Rationale**: Tool system complete and tested (195 tests passing), ready for integration
- **Migration**: Phase 1 (ConversationChain) ‚Üí Phase 2 (ReAct with tools) ‚Üí Phase 3 (LangGraph multi-agent)

## Next Steps

**‚úÖ Phase 2 Complete**: All RAG components implemented and tested
- ‚úÖ Embeddings: sentence-transformers with singleton caching (31 tests passing)
- ‚úÖ Indexing: File discovery, AST chunking, FAISS index building (9 tests passing)
- ‚úÖ Retrieval: Semantic search with ranking and threshold filtering (8 tests passing)
- ‚úÖ Tools: 4 file operations + 1 RAG search tool (147 file ops + 8 retrieval = 155 tool tests)
- **Total**: 195 tests passing (147 file ops + 48 RAG)

**‚è≥ Phase 3 (CURRENT)**: Model Compatibility & CLI Enhancement

1. **Model Compatibility Resolution** (Step 5 - HIGH PRIORITY)
   - ‚è≥ **Option A**: Implement prompt-based ReAct agent for small models (recommended)
     - Create custom ReAct loop using prompts instead of `bind_tools()`
     - Compatible with qwen2.5-coder:1.5b and other small models
     - Uses text-based tool descriptions and output parsing
   - ‚è≥ **Option B**: Document Claude/Gemini as required for tool mode
     - Update .env.example to default to `ENABLE_TOOLS=False`
     - Add clear documentation about model requirements
     - Provide instructions for using with Claude API
   - ‚è≥ Test tool functionality with compatible models (Claude Haiku, Gemini Flash)

2. **CLI Integration** (Step 6 - IN PROGRESS)
   - ‚è≥ Add `index` command to build/rebuild codebase index
   - ‚è≥ Add `info` subcommand to show index statistics (size, files, last updated)
   - ‚è≥ Add tool execution indicators (show which tool is running)
   - ‚è≥ Update help messages to document new commands
   - ‚è≥ Add progress indicators for indexing operations
   - ‚è≥ Improve error messages for common issues

3. **Testing & Validation** (Step 7 - IN PROGRESS)
   - ‚úÖ LangGraph agent structure tests (COMPLETE)
   - ‚ö†Ô∏è End-to-end tool tests (blocked by model compatibility)
   - ‚úÖ Multi-turn conversation tests (COMPLETE - simple mode)
   - ‚è≥ Performance benchmarks for RAG search
   - ‚è≥ Load testing with large codebases (10k+ files)
   - ‚è≥ Error recovery and edge case testing

4. **Production Readiness** (Step 8 - TODO)
   - Performance optimization (caching, lazy loading)
   - Logging and monitoring improvements
   - Documentation updates (user guide, troubleshooting)
   - Cloud deployment guide updates
   - Team rollout plan

## Brainstorm Ideas
- Implement streaming responses for real-time output
- Add multi-agent collaboration patterns
- Create interactive debugging mode
## Future Enhancements (Cloud LLM Optimized)

### I. Advanced Agent Intelligence & Autonomy (Leveraging Cloud LLMs)

1. **Autonomous Feature Development & Refactoring:**
   - End-to-End Feature Implementation: Enable the agent to receive high-level feature requests, autonomously plan, generate, and integrate code
   - Proactive Code Quality Improvement: Agent identifies and suggests refactoring opportunities based on code analysis

2. **Sophisticated Multi-Agent Workflows (LangGraph):**
   - Specialized Collaborative Agents: Architect, Coder, QA Agent, Reviewer roles collaborating on complex tasks
   - Dynamic Task Management: Adaptive workflows with error recovery and dependency management

3. **Integrated Code Execution & Automated Testing:**
   - Secure Sandboxed Execution Tool: Execute code in isolated Docker environments for verification and debugging
   - Intelligent Test Generation & Validation: Agent generates comprehensive unit, integration, and end-to-end tests

4. **Comprehensive Git & Version Control Integration:**
   - Full Git Toolset: Custom tool with git status, diff, add, commit, branch, checkout, pull, push operations
   - Contextual Change Management: Agent understands change intent and creates meaningful, conventional commits

5. **Advanced Database Interaction & Schema Management:**
   - Natural Language to SQL & Execution: Translate complex queries into optimized SQL for various databases
   - Schema Analysis & Evolution: Analyze schemas, suggest improvements, identify bottlenecks, propose migrations

### II. Enhanced User Experience & CLI Robustness

1. **Real-time, Transparent CLI Interaction:**
   - Streaming Responses: Implement LangChain streaming callbacks for dynamic real-time output
   - Clear Tool Execution Indicators: Display which tool the agent is currently using (e.g., "Executing ReadFileTool...")
   - Consistent Rich Console Output: Route all messages through Rich console for unified styled UI
   - Progress Indicators: Integrate tqdm for visual feedback on time-consuming operations like indexing

2. **Comprehensive CLI Management Commands:**
   - Dedicated index Command: Add `agent index [path] --force --incremental` for RAG index management
   - Enhanced info Command: Display detailed model info, RAG statistics, and tool status across all providers

3. **Robust Error Handling & Debugging Tools:**
   - Actionable Error Messages: Provide specific, context-aware guidance for resolving issues
   - Interactive Debugging Mode: Allow users to step through agent thought process and inspect tool I/O

### III. RAG System Performance & Accuracy Optimizations

1. **RAG Retriever Performance & Caching:**
   - Singleton Retriever Instance: Ensure FAISS index loads once and reuses across searches
   - Query Result Caching: Implement lru_cache for frequently asked queries

2. **Refined Similarity Score Calculation:**
   - Mathematically Accurate Cosine Similarity: Use `1 - (dist**2 / 2)` for normalized vectors

3. **Configurable RAG Parameters:**
   - Dynamic top_k and Threshold: Make retrieval parameters configurable via settings or tool input

4. **Advanced Chunking Strategies:**
   - Semantic Chunking: Group code by semantic relatedness rather than just AST structure
   - Dependency-Aware Chunking: Include dependent functions and classes in chunks for richer context[ This is a test ]