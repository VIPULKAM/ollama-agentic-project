# AI Coding Agent Configuration - Version 2.0 (Multi-Provider Support)

# LLM Provider Selection
# Options: "ollama" (self-hosted), "claude" (Anthropic API), "hybrid" (smart routing)
LLM_PROVIDER=ollama

# Ollama Configuration (for "ollama" or "hybrid" mode)
OLLAMA_BASE_URL=http://localhost:11434
MODEL_NAME=qwen2.5-coder:1.5b

# Claude/Anthropic Configuration (for "claude" or "hybrid" mode)
# Get your API key from: https://console.anthropic.com/
ANTHROPIC_API_KEY=your-api-key-here

# Model selection (options: claude-3-haiku-20240307, claude-3-5-sonnet-20241022)
CLAUDE_MODEL=claude-3-haiku-20240307

# Model Parameters
TEMPERATURE=0.1
MAX_TOKENS=2000

# Memory Configuration
MAX_HISTORY_LENGTH=10

# CLI Configuration
CLI_THEME=monokai
STREAM_OUTPUT=true

# For cloud deployment (change OLLAMA_BASE_URL to your AWS server)
# OLLAMA_BASE_URL=https://ai-server.yourcompany.com

# Hybrid Mode Configuration
# Queries containing these keywords will route to Claude for better quality
# CLAUDE_KEYWORDS=architecture,design pattern,refactor,optimize,security,best practice,review,compare
