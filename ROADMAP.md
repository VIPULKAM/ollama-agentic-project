# AI Coding Agent - Development Roadmap

**Last Updated**: December 4, 2024
**Current Version**: Production-ready with LangGraph + Gemini 2.5 Flash
**Status**: 195 tests passing, 5 tools operational

---

## üìä Current System State

### ‚úÖ What's Working
- **LangGraph Agent**: Multi-step tool execution with Gemini 2.5 Flash
- **5 Operational Tools**: read_file, write_file, list_directory, search_code, rag_search
- **Test Coverage**: 195 tests passing (147 file ops + 48 RAG)
- **Multi-Provider Support**: Ollama, Claude, Gemini with smart routing
- **FAISS RAG System**: Semantic codebase search with AST-based chunking
- **Tool Summary**: Automatic generation when agent returns empty responses

### ‚ö†Ô∏è Current Limitations
1. **Large File Handling**: Files > 10KB cause context overflow in multi-tool workflows
2. **No Streaming**: User waits without real-time feedback (30+ seconds for complex tasks)
3. **Limited Error Recovery**: Single tool failure can crash entire workflow
4. **No Task Planning**: Agent tries to do everything in one pass
5. **Context Window Constraints**: CLAUDE.md (40KB) can't be updated via agent
6. **Manual Git Operations**: No version control integration

### üéØ Key Metrics
- **File Size Limit**: ~10KB for reliable multi-tool operations
- **MAX_TOKENS**: 8192 (Gemini 2.5 Flash maximum)
- **Test Coverage**: 195 tests across 16 test files
- **Tools Available**: 5 (4 file ops + 1 RAG)
- **Supported Providers**: 3 (Ollama, Claude, Gemini)

---

## üöÄ Development Phases

## Phase 1: Quick Wins (1-2 Days)
*Immediate improvements with high impact*

### 1. Streaming Responses ‚≠ê HIGH PRIORITY
**Problem**: User sees "Agent thinking..." with no feedback for 30+ seconds

**Solution**: Implement LangChain streaming callbacks
```python
# src/agent/agent.py
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler

# In _setup_langgraph_agent():
self.langgraph_app = create_react_agent(
    self.llm,
    self.tools,
    checkpointer=self.checkpointer,
    stream_mode="updates"  # Enable streaming
)

# In ask() method:
async for chunk in self.langgraph_app.stream({"messages": messages}):
    yield chunk  # Stream to CLI
```

**CLI Integration** (src/cli/main.py):
```python
# Replace console.status() with streaming handler
for chunk in agent.ask_stream(query):
    console.print(chunk, end="")
```

**Benefits**:
- Real-time feedback during tool execution
- Shows "Reading file...", "Writing file...", "Searching codebase..."
- Better perceived performance
- User can cancel long operations

**Effort**: 2-3 hours
**Impact**: High (UX improvement)
**Complexity**: Low

---

### 2. Smart File Chunking for Large Files ‚≠ê HIGH PRIORITY
**Problem**: Can't update CLAUDE.md (40KB) - agent reads but runs out of context

**Solution**: Add intelligent file section updating

**New Tool**: `UpdateFileSectionTool`
```python
# src/agent/tools/file_ops.py

class UpdateFileSectionInput(BaseModel):
    path: str = Field(description="File path")
    start_marker: str = Field(description="Section start marker (e.g., '## Future Enhancements')")
    end_marker: Optional[str] = Field(description="Section end marker (next ##, or EOF)")
    new_content: str = Field(description="New content for this section")
    mode: str = Field(default="replace", description="replace, append, prepend")

class UpdateFileSectionTool(BaseTool):
    name = "update_file_section"
    description = "Update specific section of a file using markers"
    args_schema = UpdateFileSectionInput

    def _run(self, path: str, start_marker: str, end_marker: Optional[str],
             new_content: str, mode: str = "replace") -> str:
        # 1. Read file
        # 2. Find start_marker line
        # 3. Find end_marker (or next section/EOF)
        # 4. Replace/append/prepend content in that section
        # 5. Write back with backup
        # 6. Return diff preview
```

**Usage Example**:
```python
agent.ask("""
Update the '## Future Enhancements' section in CLAUDE.md.
Replace it with: [new content]
""")

# Agent uses: update_file_section(
#   path="CLAUDE.md",
#   start_marker="## Future Enhancements",
#   end_marker="## Next Steps",  # Auto-detected
#   new_content="...",
#   mode="replace"
# )
```

**Benefits**:
- Update large files without loading entire content into context
- Surgical edits to specific sections
- Works with files up to 1MB+
- Preserves file structure

**Effort**: 4-6 hours
**Impact**: High (solves major pain point)
**Complexity**: Medium

---

### 3. Enhanced CLI Commands
**Problem**: Missing useful commands for index management, tool discovery, configuration

**Add to src/cli/main.py**:

```python
# New commands:

elif query.lower().startswith("index"):
    # agent index              - Build/update RAG index
    # agent index --force      - Force full reindex
    # agent index --stats      - Show index statistics
    force = "--force" in query.lower()
    stats = "--stats" in query.lower()

    if stats:
        print_index_stats()
    else:
        run_indexing(force=force)
    continue

elif query.lower() == "tools":
    # agent tools - List available tools with descriptions
    print_available_tools(agent)
    continue

elif query.lower() == "config":
    # agent config - Show current configuration
    print_configuration(agent)
    continue

elif query.lower().startswith("history"):
    # agent history       - Show conversation history
    # agent history clear - Clear history
    if "clear" in query.lower():
        agent.clear_history()
        console.print("[yellow]‚úì History cleared[/yellow]")
    else:
        console.print(agent.get_conversation_history())
    continue
```

**Helper Functions**:
```python
def print_available_tools(agent: CodingAgent):
    """Display all available tools with descriptions."""
    table = Table(title="Available Tools")
    table.add_column("Tool", style="cyan")
    table.add_column("Description", style="white")

    for tool in agent.tools:
        table.add_row(tool.name, tool.description)

    console.print(table)

def print_index_stats():
    """Display RAG index statistics."""
    from src.rag.indexer import load_index

    index, chunks, info = load_index()

    table = Table(title="RAG Index Statistics")
    table.add_column("Metric", style="cyan")
    table.add_column("Value", style="green")

    table.add_row("Total Chunks", str(len(chunks)))
    table.add_row("Files Indexed", str(info.get("files_indexed", 0)))
    table.add_row("Index Size", f"{info.get('index_size_mb', 0):.2f} MB")
    table.add_row("Last Updated", info.get("last_updated", "Never"))

    console.print(table)

def print_configuration(agent: CodingAgent):
    """Display current agent configuration."""
    from src.config.settings import settings

    table = Table(title="Agent Configuration")
    table.add_column("Setting", style="cyan")
    table.add_column("Value", style="green")

    table.add_row("Provider", settings.LLM_PROVIDER)
    table.add_row("Model", settings.GEMINI_MODEL if settings.LLM_PROVIDER == "gemini" else settings.MODEL_NAME)
    table.add_row("Temperature", str(settings.TEMPERATURE))
    table.add_row("Max Tokens", str(settings.MAX_TOKENS))
    table.add_row("Tools Enabled", str(settings.ENABLE_TOOLS))
    table.add_row("RAG Enabled", str(settings.ENABLE_RAG))

    console.print(table)
```

**Benefits**:
- Better discoverability
- Easier debugging
- Self-documenting interface
- User can explore capabilities

**Effort**: 3-4 hours
**Impact**: Medium (UX improvement)
**Complexity**: Low

---

## Phase 2: Core Enhancements (3-5 Days)
*Foundational improvements for reliability and capability*

### 4. Error Recovery & Retry Logic
**Problem**: Single tool failure crashes entire workflow

**Solution**: Add retry mechanism with exponential backoff

```python
# src/agent/agent.py

class AgentExecutionError(Exception):
    """Custom exception for agent execution failures."""
    pass

def _execute_tool_with_retry(
    self,
    tool_name: str,
    tool_input: dict,
    max_retries: int = 3,
    backoff_factor: float = 2.0
) -> str:
    """Execute tool with exponential backoff retry."""
    import time

    last_error = None

    for attempt in range(max_retries):
        try:
            tool = self.tools.get(tool_name)
            if not tool:
                raise AgentExecutionError(f"Tool '{tool_name}' not found")

            result = tool.invoke(tool_input)

            # Log success
            logger.info(f"Tool '{tool_name}' succeeded on attempt {attempt + 1}")
            return result

        except Exception as e:
            last_error = e

            if attempt < max_retries - 1:
                wait_time = backoff_factor ** attempt
                logger.warning(
                    f"Tool '{tool_name}' failed (attempt {attempt + 1}/{max_retries}). "
                    f"Retrying in {wait_time}s... Error: {str(e)}"
                )
                time.sleep(wait_time)
            else:
                logger.error(
                    f"Tool '{tool_name}' failed after {max_retries} attempts. "
                    f"Last error: {str(e)}"
                )

    raise AgentExecutionError(
        f"Tool '{tool_name}' failed after {max_retries} attempts: {str(last_error)}"
    )
```

**Integration with LangGraph**:
```python
# Wrap tool execution in custom node
from langgraph.graph import StateGraph

def create_agent_with_retry():
    # Custom tool execution node with retry logic
    def tool_execution_node(state):
        messages = state["messages"]
        last_message = messages[-1]

        if hasattr(last_message, "tool_calls"):
            results = []
            for tool_call in last_message.tool_calls:
                result = _execute_tool_with_retry(
                    tool_call["name"],
                    tool_call["args"]
                )
                results.append(ToolMessage(content=result, tool_call_id=tool_call["id"]))

            return {"messages": results}

    # Build custom graph
    workflow = StateGraph()
    workflow.add_node("agent", agent_node)
    workflow.add_node("tools", tool_execution_node)  # Custom node with retry
    ...
```

**Benefits**:
- Resilient to transient errors (network, rate limits)
- Better success rate for operations
- Detailed error logging for debugging
- Graceful degradation

**Effort**: 4-5 hours
**Impact**: Medium (reliability)
**Complexity**: Medium

---

### 5. Multi-Step Task Planning
**Problem**: Agent tries to do everything in one pass, fails on complex tasks

**Solution**: Add planning phase before execution

**Architecture**:
```
User Query
    ‚Üì
Planner Agent (breaks down into steps)
    ‚Üì
[Step 1] ‚Üí [Step 2] ‚Üí [Step 3] ‚Üí ...
    ‚Üì
Executor Agent (executes each step)
    ‚Üì
Synthesizer Agent (combines results)
    ‚Üì
User Response
```

**Implementation**:
```python
# src/agent/planner.py

class TaskPlanner:
    """Plans multi-step tasks before execution."""

    def create_plan(self, query: str, context: dict) -> List[Step]:
        """Break down query into executable steps."""

        # Use LLM to create plan
        planning_prompt = f"""
        Break down this task into clear, executable steps:

        Task: {query}

        Available tools: {', '.join(tool.name for tool in self.tools)}

        Return a numbered list of steps, each specifying:
        1. What to do
        2. Which tool to use
        3. Expected output

        Example:
        1. Read config.json to understand current structure (tool: read_file)
        2. Search for similar patterns in codebase (tool: search_code)
        3. Update config.json with new values (tool: write_file)
        """

        response = self.llm.invoke(planning_prompt)
        steps = self._parse_plan(response.content)
        return steps

    def _parse_plan(self, plan_text: str) -> List[Step]:
        """Parse LLM response into structured steps."""
        # Extract numbered steps
        # Identify tool for each step
        # Create Step objects
        ...

class Step:
    """Represents a single step in execution plan."""
    number: int
    description: str
    tool: str
    input_schema: dict
    expected_output: str
```

**Execution**:
```python
# src/agent/executor.py

class StepExecutor:
    """Executes plan steps sequentially."""

    def execute_plan(self, plan: List[Step]) -> List[StepResult]:
        results = []
        context = {}  # Shared context across steps

        for step in plan:
            console.print(f"\n[cyan]Step {step.number}:[/cyan] {step.description}")

            # Execute step with retry
            result = self._execute_step_with_retry(step, context)
            results.append(result)

            # Update context for next step
            context[f"step_{step.number}_output"] = result.output

            # Check if should continue
            if result.status == "failed" and step.critical:
                console.print(f"[red]Critical step failed. Aborting plan.[/red]")
                break

        return results
```

**Benefits**:
- Handle complex multi-file refactoring
- Better success rate on ambiguous tasks
- User can review plan before execution
- Easier debugging (see which step failed)

**Effort**: 6-8 hours
**Impact**: High (enables complex tasks)
**Complexity**: High

---

### 6. Context-Aware File Reading
**Problem**: Always reads entire file even when only need snippet

**Solution**: Smart file reading with line ranges and pattern matching

```python
# src/agent/tools/smart_file_ops.py

class SmartReadFileInput(BaseModel):
    path: str = Field(description="File path")
    mode: str = Field(
        default="auto",
        description="Reading mode: 'full', 'lines', 'pattern', 'auto'"
    )
    start_line: Optional[int] = Field(description="Start line (1-indexed)")
    end_line: Optional[int] = Field(description="End line (inclusive)")
    pattern: Optional[str] = Field(description="Regex pattern to search for")
    context_lines: int = Field(default=5, description="Lines of context around matches")
    max_size_kb: int = Field(default=10, description="Max file size for full read")

class SmartReadFileTool(BaseTool):
    name = "read_file_smart"
    description = """
    Read file with intelligent context management.
    - For small files (< 10KB): reads full content
    - For large files: requires line range or pattern
    - Pattern mode: returns matches with surrounding context
    """
    args_schema = SmartReadFileInput

    def _run(
        self,
        path: str,
        mode: str = "auto",
        start_line: Optional[int] = None,
        end_line: Optional[int] = None,
        pattern: Optional[str] = None,
        context_lines: int = 5,
        max_size_kb: int = 10
    ) -> str:
        validated_path = validate_path(path)
        file_size_kb = validated_path.stat().st_size / 1024

        # Auto mode: decide based on file size
        if mode == "auto":
            if file_size_kb <= max_size_kb:
                mode = "full"
            elif pattern:
                mode = "pattern"
            elif start_line and end_line:
                mode = "lines"
            else:
                return f"‚ö†Ô∏è File too large ({file_size_kb:.1f}KB > {max_size_kb}KB). " \
                       f"Specify line range or pattern to search."

        # Full mode
        if mode == "full":
            return validated_path.read_text()

        # Lines mode
        if mode == "lines":
            lines = validated_path.read_text().split('\n')
            selected = lines[start_line-1:end_line]
            return '\n'.join(f"{i}: {line}" for i, line in enumerate(selected, start=start_line))

        # Pattern mode
        if mode == "pattern":
            import re
            lines = validated_path.read_text().split('\n')
            matches = []

            for i, line in enumerate(lines, 1):
                if re.search(pattern, line):
                    # Include context
                    start = max(0, i - context_lines - 1)
                    end = min(len(lines), i + context_lines)
                    context = lines[start:end]

                    matches.append(
                        f"\n--- Match at line {i} ---\n" +
                        '\n'.join(f"{j}: {l}" for j, l in enumerate(context, start=start+1))
                    )

            if matches:
                return '\n'.join(matches)
            else:
                return f"No matches found for pattern: {pattern}"
```

**Benefits**:
- Reduces context usage by 80-90% for large files
- Can search CLAUDE.md for specific sections
- Agent can work with files up to 1MB+
- Faster response times

**Effort**: 5-6 hours
**Impact**: High (enables large file handling)
**Complexity**: Medium

---

## Phase 3: Advanced Features (1-2 Weeks)
*Game-changing capabilities for autonomous workflows*

### 7. Git Integration Tools ‚≠ê HIGH IMPACT
**Add**: Complete git workflow automation

**New Tools**:

```python
# src/agent/tools/git_tools.py

class GitStatusTool(BaseTool):
    name = "git_status"
    description = "Show git status, unstaged/staged changes, current branch"

    def _run(self) -> str:
        # Execute: git status --porcelain
        # Parse output
        # Return formatted status

class GitDiffTool(BaseTool):
    name = "git_diff"
    description = "Show changes in working directory or staged area"

    def _run(self, staged: bool = False, file_path: Optional[str] = None) -> str:
        # git diff [--staged] [file_path]
        # Return unified diff

class GitCommitTool(BaseTool):
    name = "git_commit"
    description = "Stage and commit changes with AI-generated message"

    def _run(self, files: List[str], message: Optional[str] = None) -> str:
        # If no message: analyze diff and generate conventional commit message
        # git add [files]
        # git commit -m "[message]"

class GitBranchTool(BaseTool):
    name = "git_branch"
    description = "Create, switch, list, or delete branches"

    def _run(self, action: str, branch_name: Optional[str] = None) -> str:
        # git branch [options]
        # git checkout [branch]

class GitLogTool(BaseTool):
    name = "git_log"
    description = "Show commit history"

    def _run(self, limit: int = 10, file_path: Optional[str] = None) -> str:
        # git log --oneline -n [limit] [file_path]
```

**Usage Example**:
```python
User: "Create a new feature branch and commit my changes"

Agent:
1. Uses git_status to see changes
2. Uses git_branch(action="create", branch_name="feature/new-feature")
3. Analyzes changes with git_diff
4. Generates commit message
5. Uses git_commit(files=["src/agent/agent.py"], message="Add retry logic...")
6. Responds: "‚úì Created branch 'feature/new-feature' and committed changes"
```

**Benefits**:
- Full version control from the agent
- AI-generated conventional commit messages
- Autonomous feature branch workflows
- No manual git commands needed

**Effort**: 6-8 hours
**Impact**: Very High (game-changer)
**Complexity**: Medium

---

### 8. Code Execution Sandbox ‚≠ê HIGH IMPACT
**Add**: Safe code execution for testing and validation

**Architecture**:
```
Agent generates code
    ‚Üì
ExecuteCodeTool (Docker container)
    ‚Üì
Result (stdout, stderr, return value)
    ‚Üì
Agent validates & iterates if needed
```

**Implementation**:
```python
# src/agent/tools/code_execution.py

import docker
import tempfile
from pathlib import Path

class ExecuteCodeInput(BaseModel):
    code: str = Field(description="Python code to execute")
    timeout: int = Field(default=30, description="Execution timeout in seconds")
    requirements: List[str] = Field(default=[], description="pip packages to install")

class ExecuteCodeTool(BaseTool):
    name = "execute_code"
    description = "Execute Python code in isolated Docker container"
    args_schema = ExecuteCodeInput

    def __init__(self):
        self.docker_client = docker.from_env()
        self.image = "python:3.11-slim"

    def _run(self, code: str, timeout: int = 30, requirements: List[str] = None) -> str:
        # Create temp directory for code
        with tempfile.TemporaryDirectory() as tmpdir:
            code_path = Path(tmpdir) / "code.py"
            code_path.write_text(code)

            # Create requirements.txt if needed
            if requirements:
                req_path = Path(tmpdir) / "requirements.txt"
                req_path.write_text('\n'.join(requirements))

            # Run in Docker container
            try:
                container = self.docker_client.containers.run(
                    self.image,
                    command=f"bash -c 'cd /code && "
                            f"{'pip install -r requirements.txt &&' if requirements else ''}"
                            f"python code.py'",
                    volumes={tmpdir: {'bind': '/code', 'mode': 'ro'}},
                    network_mode="none",  # No network access
                    mem_limit="256m",     # Memory limit
                    cpu_quota=50000,      # CPU limit
                    timeout=timeout,
                    remove=True,
                    detach=False
                )

                output = container.decode('utf-8')
                return f"‚úì Code executed successfully:\n{output}"

            except docker.errors.ContainerError as e:
                return f"‚úó Execution error:\n{e.stderr.decode('utf-8')}"

            except docker.errors.APIError as e:
                return f"‚úó Docker error: {str(e)}"
```

**Benefits**:
- Agent can test code it writes
- Verify functionality before committing
- Interactive debugging
- Safe experimentation

**Effort**: 8-10 hours
**Impact**: Very High (enables test-driven development)
**Complexity**: High

---

### 9. Multi-Agent Orchestration
**Upgrade**: Specialized agents for different tasks

**Architecture**:
```
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ   Router    ‚îÇ
                    ‚îÇ    Agent    ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚îÇ
          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
          ‚îÇ                ‚îÇ                ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Architect ‚îÇ    ‚îÇ  Coder  ‚îÇ    ‚îÇ Reviewer  ‚îÇ
    ‚îÇ   Agent   ‚îÇ    ‚îÇ  Agent  ‚îÇ    ‚îÇ   Agent   ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚îÇ                ‚îÇ                ‚îÇ
          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ Synthesizer ‚îÇ
                    ‚îÇ    Agent    ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Implementation**:
```python
# src/agent/multi_agent.py

from langgraph.graph import StateGraph, END

class ArchitectAgent:
    """Makes high-level design decisions."""

    def __call__(self, state: dict) -> dict:
        # Analyze requirements
        # Design architecture
        # Return design document
        ...

class CoderAgent:
    """Implements code based on design."""

    def __call__(self, state: dict) -> dict:
        # Read design from state
        # Generate code
        # Return implementation
        ...

class ReviewerAgent:
    """Reviews code for quality, security, best practices."""

    def __call__(self, state: dict) -> dict:
        # Read code from state
        # Check for issues
        # Return review comments
        ...

class TesterAgent:
    """Generates and runs tests."""

    def __call__(self, state: dict) -> dict:
        # Read code from state
        # Generate tests
        # Execute tests
        # Return test results
        ...

def create_multi_agent_system():
    """Create LangGraph multi-agent workflow."""

    workflow = StateGraph()

    # Add agents as nodes
    workflow.add_node("architect", ArchitectAgent())
    workflow.add_node("coder", CoderAgent())
    workflow.add_node("reviewer", ReviewerAgent())
    workflow.add_node("tester", TesterAgent())

    # Define workflow
    workflow.set_entry_point("architect")
    workflow.add_edge("architect", "coder")
    workflow.add_edge("coder", "reviewer")

    # Conditional edge: if review fails, go back to coder
    workflow.add_conditional_edges(
        "reviewer",
        lambda state: "coder" if state.get("review_issues") else "tester"
    )

    workflow.add_edge("tester", END)

    return workflow.compile()
```

**Benefits**:
- Better code quality (specialist agents)
- Parallel work on different aspects
- Built-in review process
- Scalable to more agents

**Effort**: 2-3 days
**Impact**: Very High (production-grade code generation)
**Complexity**: Very High

---

### 10. Enhanced RAG System
**Optimizations** for better performance and accuracy

**Improvements**:

```python
# 1. Singleton Retriever (avoid disk I/O)
# src/rag/retriever.py

@lru_cache(maxsize=1)
def get_singleton_retriever(index_path: str) -> SemanticRetriever:
    """Load retriever once and reuse."""
    return SemanticRetriever(index_path)

# 2. Better Similarity Calculation
# src/rag/retriever.py

def search(self, query: str, top_k: int = 5) -> List[SearchResult]:
    # Old: similarity = 1.0 / (1.0 + dist)
    # New: similarity = 1.0 - (dist**2 / 2)  # More accurate for normalized vectors
    ...

# 3. Configurable Parameters
# src/config/settings.py

RAG_TOP_K: int = 5
RAG_SIMILARITY_THRESHOLD: float = 0.5
RAG_ENABLE_CACHING: bool = True
RAG_CACHE_SIZE: int = 100

# 4. Query Result Caching
# src/rag/retriever.py

from functools import lru_cache

class SemanticRetriever:
    @lru_cache(maxsize=100)
    def search_cached(self, query: str, top_k: int) -> Tuple[SearchResult]:
        """Cache search results for repeated queries."""
        results = self.search(query, top_k)
        return tuple(results)  # Convert to tuple for hashability

# 5. Semantic Chunking (beyond AST)
# src/rag/chunker.py

class SemanticChunker:
    """Chunk code by semantic relatedness using embeddings."""

    def chunk_by_semantics(self, code: str) -> List[CodeChunk]:
        # 1. Split into sentences/blocks
        # 2. Generate embeddings for each
        # 3. Cluster by similarity
        # 4. Create chunks from clusters
        ...

# 6. Dependency-Aware Chunking
# src/rag/chunker.py

class DependencyAwareChunker:
    """Include dependencies in chunks for better context."""

    def chunk_with_deps(self, code: str) -> List[CodeChunk]:
        # 1. Parse AST
        # 2. Find function/class definitions
        # 3. Trace dependencies (imports, calls)
        # 4. Include relevant dependencies in chunk
        ...
```

**Benefits**:
- 5-10x faster search (singleton + caching)
- More accurate similarity scores
- Better context in search results
- Configurable for different use cases

**Effort**: 1-2 days
**Impact**: Medium (performance improvement)
**Complexity**: Medium

---

## Phase 4: Production Readiness (2-3 Weeks)
*Enterprise-grade features for deployment*

### 11. Observability & Monitoring

**LangSmith Integration**:
```python
# src/agent/observability.py

from langsmith import Client
from langsmith.run_helpers import traceable

class AgentObservability:
    def __init__(self):
        self.client = Client()

    @traceable(name="agent_query")
    def track_query(self, query: str, response: str, tools_used: List[str]):
        """Track agent queries for analysis."""
        # Auto-logged to LangSmith dashboard
        pass

    def log_metrics(self):
        """Log custom metrics."""
        metrics = {
            "tool_execution_time": {...},
            "token_usage": {...},
            "error_rate": {...},
            "success_rate": {...}
        }
        # Send to monitoring service
```

**Metrics to Track**:
- Tool execution times
- Token usage per query
- Error rates by tool
- Success/failure ratios
- User satisfaction (thumbs up/down)
- Query complexity distribution

**Effort**: 5-7 days
**Impact**: Medium (production monitoring)
**Complexity**: Medium

---

### 12. Conversation Persistence

**Replace In-Memory with Redis/PostgreSQL**:
```python
# src/agent/persistence.py

from langchain_redis import RedisChatMessageHistory
from langchain_postgres import PostgresChatMessageHistory

class PersistentAgent(CodingAgent):
    def __init__(self, storage: str = "redis", **kwargs):
        super().__init__(**kwargs)

        if storage == "redis":
            self.message_history = RedisChatMessageHistory(
                session_id=self.session_id,
                url="redis://localhost:6379/0"
            )
        elif storage == "postgres":
            self.message_history = PostgresChatMessageHistory(
                session_id=self.session_id,
                connection_string="postgresql://user:pass@localhost/db"
            )

    def _get_session_history(self, session_id: str):
        """Get persistent history instead of in-memory."""
        return self.message_history
```

**Benefits**:
- Persist across restarts
- Multi-session support
- Session analytics
- User history browsing

**Effort**: 3-4 days
**Impact**: Medium (better UX)
**Complexity**: Low

---

### 13. Web API & UI

**FastAPI + LangServe Deployment**:
```python
# src/api/server.py

from fastapi import FastAPI
from langserve import add_routes
from src.agent.agent import CodingAgent

app = FastAPI(title="AI Coding Agent API")

agent = CodingAgent()

# Add LangServe routes
add_routes(
    app,
    agent.langgraph_app,
    path="/agent",
    enable_feedback_endpoint=True,
    enable_public_trace_link_endpoint=True
)

@app.get("/health")
def health():
    return {"status": "healthy"}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

**React Frontend**:
```typescript
// frontend/src/App.tsx

import { RemoteRunnable } from "langchain/runnables/remote";

const agent = new RemoteRunnable({
  url: "http://localhost:8000/agent"
});

function ChatInterface() {
  const [messages, setMessages] = useState([]);

  const sendMessage = async (query: string) => {
    // Stream response
    const stream = await agent.stream({
      messages: [{ role: "user", content: query }]
    });

    for await (const chunk of stream) {
      // Update UI with streaming response
      updateMessages(chunk);
    }
  };

  return <div>{/* Chat UI */}</div>;
}
```

**Benefits**:
- Web-based interface
- API for integrations
- Mobile access
- Team collaboration

**Effort**: 2-3 weeks
**Impact**: High (accessibility)
**Complexity**: High

---

## üìÖ Recommended Implementation Order

### Week 1: Quick Wins
- ‚úÖ Day 1-2: Streaming responses
- ‚úÖ Day 2-3: Smart file chunking
- ‚úÖ Day 4-5: Enhanced CLI commands

### Week 2-3: Core Enhancements
- ‚è≥ Day 6-8: Error recovery & retry logic
- ‚è≥ Day 9-12: Multi-step task planning
- ‚è≥ Day 13-15: Context-aware file reading

### Week 4-6: Advanced Features
- ‚è≥ Day 16-18: Git integration tools
- ‚è≥ Day 19-23: Code execution sandbox
- ‚è≥ Day 24-30: Multi-agent orchestration
- ‚è≥ Day 31-33: Enhanced RAG system

### Week 7-10: Production Readiness
- ‚è≥ Week 7-8: Observability & monitoring
- ‚è≥ Week 8-9: Conversation persistence
- ‚è≥ Week 9-10: Web API & UI

---

## üéØ Success Metrics

**Phase 1 Success Criteria**:
- [ ] Streaming responses show real-time progress
- [ ] Can update CLAUDE.md (40KB) reliably
- [ ] Users can discover tools via `agent tools` command

**Phase 2 Success Criteria**:
- [ ] < 5% failure rate on tool executions
- [ ] Can handle 5+ step tasks autonomously
- [ ] Can read/update files up to 100KB

**Phase 3 Success Criteria**:
- [ ] Can create feature branch, commit, push autonomously
- [ ] Can execute and validate generated code
- [ ] Multiple agents collaborate on complex tasks
- [ ] RAG search is < 100ms

**Phase 4 Success Criteria**:
- [ ] 99% uptime for API
- [ ] Conversation history persists indefinitely
- [ ] Web UI deployed and accessible
- [ ] Monitoring dashboard shows key metrics

---

## üîÑ Continuous Improvements

**Ongoing**:
- Add more specialized tools as needs arise
- Expand test coverage (target: 95%+)
- Performance optimizations based on metrics
- User feedback integration
- Documentation updates

**Future Considerations**:
- VS Code extension
- GitHub integration (PR reviews, issue triage)
- Slack/Discord bot
- Cloud deployment (AWS/GCP/Azure)
- Enterprise features (SSO, RBAC, audit logs)

---

## üìù Notes

**Last Updated**: December 4, 2024
**Maintained By**: AI Coding Agent Team
**Review Frequency**: Monthly or after major milestones

For current implementation status, see [CLAUDE.md](./CLAUDE.md)
For architecture details, see [ARCHITECTURE.md](./ARCHITECTURE.md)
